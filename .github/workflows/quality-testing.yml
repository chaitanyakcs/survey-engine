name: Quality Testing and Monitoring

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master, develop ]
  schedule:
    # Run quality checks daily at 6 AM UTC
    - cron: '0 6 * * *'

env:
  PYTHON_VERSION: '3.9'
  NODE_VERSION: '18'

jobs:
  # Backend testing
  backend-tests:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_survey_engine
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install pytest pytest-cov pytest-asyncio

    - name: Set up test environment
      run: |
        export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/test_survey_engine"
        export REDIS_URL="redis://localhost:6379"
        export REPLICATE_API_TOKEN="test_token"
        export OPENAI_API_KEY="test_key"
        export DEBUG="true"

    - name: Run database migrations
      run: |
        export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/test_survey_engine"
        alembic upgrade head

    - name: Run comprehensive test suite
      run: |
        export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/test_survey_engine"
        export REDIS_URL="redis://localhost:6379"
        export REPLICATE_API_TOKEN="test_token"
        pytest tests/ \
          --cov=src \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --cov-fail-under=80 \
          -v \
          --tb=short

    - name: Run specific core service tests
      run: |
        export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/test_survey_engine"
        pytest tests/test_generation_service.py \
               tests/test_prompt_service.py \
               tests/test_document_parser.py \
               -v --tb=short

    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: backend
        name: backend-coverage

    - name: Archive test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: backend-test-results
        path: |
          htmlcov/
          coverage.xml
          pytest.xml

  # Frontend testing
  frontend-tests:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json

    - name: Install frontend dependencies
      working-directory: ./frontend
      run: npm ci

    - name: Run frontend tests
      working-directory: ./frontend
      run: |
        npm run test -- --coverage --watchAll=false
        npm run test:e2e

    - name: Run TypeScript type checking
      working-directory: ./frontend
      run: npm run type-check

    - name: Run linting
      working-directory: ./frontend
      run: npm run lint

    - name: Build frontend
      working-directory: ./frontend
      run: npm run build

    - name: Upload frontend coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./frontend/coverage/coverage-final.json
        flags: frontend
        name: frontend-coverage

  # Quality regression testing
  quality-regression-tests:
    runs-on: ubuntu-latest
    needs: [backend-tests]
    if: github.event_name == 'schedule' || github.event_name == 'push'

    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: quality_test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest

    - name: Run quality regression detection
      run: |
        export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/quality_test_db"
        python -c "
        import asyncio
        from src.services.quality_regression_service import QualityRegressionService
        from src.database.connection import get_db

        async def test_regression_detection():
            service = QualityRegressionService()

            # Run regression detection
            alerts = await service.detect_regressions(lookback_hours=24)
            print(f'Detected {len(alerts)} quality regressions')

            # Update baselines if needed
            baselines = await service.update_baselines(period_days=30)
            print(f'Updated {len(baselines)} quality baselines')

            return len(alerts)

        alert_count = asyncio.run(test_regression_detection())

        # Fail if severe regressions detected
        if alert_count > 5:
            print(f'CRITICAL: {alert_count} quality regressions detected!')
            exit(1)
        else:
            print(f'Quality check passed: {alert_count} minor regressions')
        "

    - name: Archive quality reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: quality-regression-reports
        path: |
          quality_reports/
          *.log

  # Performance testing
  performance-tests:
    runs-on: ubuntu-latest
    needs: [backend-tests]
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')

    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: perf_test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-benchmark

    - name: Run performance tests
      run: |
        export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/perf_test_db"
        pytest tests/test_generation_service.py::TestGenerationServicePerformance \
               tests/test_prompt_service.py::TestPromptServicePerformance \
               tests/test_document_parser.py::TestDocumentParserPerformance \
               --benchmark-only \
               --benchmark-json=benchmark_results.json \
               -v

    - name: Check performance regressions
      run: |
        python -c "
        import json

        # Load benchmark results
        with open('benchmark_results.json', 'r') as f:
            results = json.load(f)

        # Check if any test exceeded acceptable thresholds
        performance_issues = []

        for benchmark in results.get('benchmarks', []):
            name = benchmark['name']
            mean_time = benchmark['stats']['mean']

            # Define performance thresholds
            thresholds = {
                'test_generate_survey': 5.0,      # 5 seconds max
                'test_large_prompt_generation': 1.0,  # 1 second max
                'test_json_extraction': 0.5,      # 0.5 seconds max
                'test_text_extraction': 2.0       # 2 seconds max
            }

            # Check threshold
            for test_pattern, threshold in thresholds.items():
                if test_pattern in name and mean_time > threshold:
                    performance_issues.append(f'{name}: {mean_time:.2f}s > {threshold}s')

        if performance_issues:
            print('PERFORMANCE REGRESSIONS DETECTED:')
            for issue in performance_issues:
                print(f'  - {issue}')
            exit(1)
        else:
            print('All performance tests passed!')
        "

    - name: Archive performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-test-results
        path: benchmark_results.json

  # Security scanning
  security-scan:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    - name: Run Bandit security scan
      run: |
        pip install bandit
        bandit -r src/ -f json -o bandit-report.json || true

    - name: Run Safety check
      run: |
        pip install safety
        safety check --json --output safety-report.json || true

    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  # Integration tests
  integration-tests:
    runs-on: ubuntu-latest
    needs: [backend-tests, frontend-tests]
    if: github.event_name == 'push'

    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: integration_test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        cd frontend && npm ci

    - name: Start backend server
      run: |
        export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/integration_test_db"
        export REDIS_URL="redis://localhost:6379"
        export REPLICATE_API_TOKEN="test_token"
        alembic upgrade head
        uvicorn src.main:app --host 0.0.0.0 --port 8000 &
        sleep 10

    - name: Run integration tests
      run: |
        # Test API endpoints
        curl -f http://localhost:8000/health || exit 1
        curl -f http://localhost:8000/api/v1/quality/overview || exit 1

        # Run end-to-end tests
        cd frontend
        npm run test:integration

    - name: Archive integration test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: |
          integration-test-results/
          *.log

  # Deployment readiness check
  deployment-check:
    runs-on: ubuntu-latest
    needs: [backend-tests, frontend-tests, integration-tests]
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master'

    steps:
    - uses: actions/checkout@v4

    - name: Check deployment readiness
      run: |
        echo "=== DEPLOYMENT READINESS CHECK ==="
        echo "âœ… Backend tests passed"
        echo "âœ… Frontend tests passed"
        echo "âœ… Integration tests passed"
        echo "ðŸš€ Ready for deployment!"

    - name: Notify deployment readiness
      if: success()
      run: |
        echo "All quality gates passed - deployment ready!"
        # Here you could integrate with deployment tools
        # or notification services (Slack, email, etc.)

  # Cleanup
  cleanup:
    runs-on: ubuntu-latest
    needs: [backend-tests, frontend-tests, quality-regression-tests, performance-tests, integration-tests]
    if: always()

    steps:
    - name: Cleanup test artifacts
      run: |
        echo "Cleaning up test artifacts..."
        # Cleanup commands if needed