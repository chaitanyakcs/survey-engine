# Cursor Rules for Survey Engine Project

## Project Overview
This is a full-stack AI-powered survey generation engine that transforms RFQs into researcher-ready surveys using:
- **Backend**: Python 3.11+ FastAPI with SQLAlchemy
- **Frontend**: React/TypeScript with Tailwind CSS and Zustand
- **Database**: PostgreSQL with pgvector extension
- **AI/ML**: Replicate API (meta/llama-3.3-70b-instruct), sentence-transformers
- **Workflow**: LangGraph StateGraph for orchestration
- **Real-Time**: WebSocket for live progress updates
- **Containerization**: Docker with docker-compose
- **Deployment**: Railway for production, local via start-local.sh
- **Evaluation**: 5-Pillar quality assessment system

## Python Version Requirement
- **Python 3.11-3.12 only** (`>=3.11,<3.13`)
- This is a hard constraint - do not use Python 3.13+ features

## Code Style & Standards

### Python Backend
- Follow PEP 8 style guidelines
- Use type hints for all function parameters and return values
- Use async/await for database operations and external API calls
- Follow FastAPI best practices for API endpoints
- Use SQLAlchemy ORM patterns consistently
- Place business logic in service classes, not in route handlers
- Use proper error handling with custom exceptions (UserFriendlyError)
- Follow the existing project structure in `src/` directory
- **Formatting**: Use `black` (line-length: 88)
- **Import Sorting**: Use `isort` (profile: "black")
- **Linting**: Use `flake8` for code quality
- **Type Checking**: Use `mypy` (strict mode with documented exemptions)

### Async/Await Guidelines
- **CRITICAL**: Always use `await` when calling async methods
- **Never call async methods without `await`** - this causes `'coroutine' object is not subscriptable` errors
- Use `@pytest.mark.asyncio` for async tests
- Use `AsyncMock()` for mocking async methods in tests
- Wrap async calls in try/except blocks for proper error handling
- Use `asyncio.wait_for()` for timeout handling
- Test both success and failure paths for async operations
- **Common async methods in this codebase**:
  - `PromptService.create_survey_generation_prompt()`
  - `PromptService.build_system_prompt()`
  - `GenerationService.generate_survey()`
  - `WorkflowService.process_rfq()`
  - All service methods starting with `async def`

### Frontend (React/TypeScript)
- Use TypeScript strict mode
- Follow React functional component patterns with hooks
- Use Tailwind CSS for styling (no inline styles)
- Implement proper error boundaries
- **State Management**: Use Zustand store (not Redux or Context API)
- Follow the existing component structure in `frontend/src/`
- Handle WebSocket connections for real-time updates
- Use ErrorClassifier for user-friendly error messages
- **Build Verification**: Always run `npm run build` in the `frontend/` directory after making frontend changes to catch TypeScript errors, build issues, and ensure the code compiles successfully before committing

### Database
- Use SQL migrations in `migrations/` directory for database schema changes
- **CRITICAL**: Execute migrations ONLY via admin API endpoints in `admin.py` - NEVER run migrations directly
- **REQUIRED**: All new migrations MUST be registered in `src/api/admin.py` in the `incremental_migrations` list within the `migrate_all` endpoint
- Migration registration pattern:
  1. Create SQL file in `migrations/` directory (e.g., `054_add_feature.sql`)
  2. Add filename to `incremental_migrations` list in `admin.py` (around line 271-283)
  3. Migrations are executed in order from this list when `/api/v1/admin/migrate-all` is called
- Follow naming conventions: snake_case for tables and columns
- Use proper foreign key relationships
- Include proper indexes for performance
- Migrations are idempotent and can be run multiple times safely
- Use pgvector for vector similarity search in production

## AI/ML Integration Patterns

### Replicate AI Service
- **Primary AI Provider**: Replicate API (meta/llama-3.3-70b-instruct)
- Use `replicate.Client` with proper API token configuration
- Check for `REPLICATE_API_TOKEN` environment variable
- Implement error handling for Replicate API failures
- Use JSON-optimized hyperparameters for structured output
- Implement LLM audit tracking for all AI calls using `@audit_llm_call` decorator
- Note: OpenAI references in error messages are generic fallback text only
- **Timeouts (2024-11-16)**: Extended to 30 minutes to prevent ReadTimeout errors
  - httpx read timeout: 1800s (30 min)
  - asyncio wait_for: 1800s (30 min)
  - connect/write/pool: 60s each
  - Rationale: Long-running LLM generations (especially regeneration) need extended timeouts

### Embeddings & Vector Search
- Use sentence-transformers (all-MiniLM-L6-v2) for vector embeddings
- Generate embeddings for RFQ text and golden examples
- Store embeddings in PostgreSQL with pgvector extension
- Use pgvector for similarity search in production (not FAISS)
- Implement similarity thresholds (default: 0.75)
- Follow tiered retrieval strategy: Golden pairs → Methodology blocks → Templates

### LLM Audit Pattern
- Use `@audit_llm_call` decorator for all AI service calls
- Track model, provider, tokens, latency, cost estimates
- Store audit records in `llm_audit` table
- Include raw responses for debugging
- Log errors and retries

## WebSocket & Real-Time Patterns

### ConnectionManager
- Manage WebSocket connections per workflow_id in `src/main.py`
- Handle connection lifecycle: connect, disconnect, send_progress
- Support multiple connections per workflow_id
- Implement proper error handling for failed sends
- Use structured progress messages with type, stage, substage, percentage

### Progress Tracking
- Use `ProgressTracker` from `src/services/progress_tracker.py` for workflow progress state
- Send progress updates at each workflow stage
- Include stage, substage, percentage, and message in updates
- Integrate with `WebSocketNotificationService` for broadcasting
- Track progress in workflow node wrappers

### Frontend Integration
- Use Zustand store for state management and WebSocket handling
- Connect WebSocket on workflow start (`/ws/survey/{workflow_id}`)
- Handle connection lifecycle and reconnection logic
- Update UI in real-time based on progress messages
- Display progress with ProgressStepper component

### Regeneration Pattern (2024-11-16)
- **Survey regeneration uses SYNCHRONOUS pattern (no WebSocket)**
- WorkflowService initialized with `connection_manager=None` for regeneration
- API endpoint waits for completion before returning
- Frontend shows simple loading state, then redirects to new survey
- Rationale: Simpler code, eliminates ReadTimeout issues, better UX
- See `REGENERATION_SIMPLIFICATION.md` for full details

## LangGraph Workflow Patterns

### StateGraph Architecture
- Use LangGraph's `StateGraph` with `SurveyGenerationState`
- Define workflow nodes: parse_rfq → retrieve_golden → build_context → generate → validate → review
- Implement progress tracking in each node wrapper function
- Use async/await for all node functions
- Pass connection_manager to nodes that need WebSocket updates

### Workflow State Management
- `SurveyGenerationState` contains all workflow data
- Include workflow_id for tracking and WebSocket updates
- Pass state between nodes immutably
- Handle validation failures with retry logic
- Store workflow state in database via `workflow_state_service`

### Workflow Node Pattern
```python
async def node_with_progress(state: SurveyGenerationState) -> Dict[str, Any]:
    progress_tracker = get_progress_tracker(state.workflow_id)
    
    # Send progress update
    progress_data = progress_tracker.get_progress_data("stage", "substage")
    await ws_client.send_progress_update(state.workflow_id, progress_data)
    
    # Execute node logic
    result = await actual_node(state)
    
    # Send completion update
    completion_data = progress_tracker.get_completion_data("stage", "completed")
    await ws_client.send_progress_update(state.workflow_id, completion_data)
    
    return result
```

## 5-Pillar Evaluation System

### Pillar Structure
- **content_validity**: Research objective coverage and alignment
- **methodological_rigor**: Survey design and methodology compliance
- **respondent_experience**: Question clarity and survey flow
- **analytical_value**: Data quality and actionability
- **business_impact**: Decision-making value and ROI

### Evaluation Modules
- Use chain-of-thought LLM-based analysis
- Implement evaluators in `evaluations/modules/`
- Return structured results with scores (0.0-1.0) and recommendations
- Integrate with `ConsolidatedRulesService` for pillar rules
- Store evaluation results in database
- Use evaluation results for quality gates

### Evaluation Pattern
```python
class PillarEvaluator:
    def __init__(self, llm_client, db_session):
        self.llm_client = llm_client
        self.db_session = db_session
        self.pillar_rules_service = ConsolidatedRulesService(db_session)
    
    async def evaluate(self, survey, rfq_text) -> PillarResult:
        # Chain-of-thought analysis
        # Return structured result with score and recommendations
```

## Service Layer Patterns

### Service Initialization
- Accept optional `db_session`, `workflow_id`, `connection_manager` parameters
- Use graceful fallback for unavailable dependencies (Redis, WebSocket, etc.)
- Log service configuration on initialization using `log_service_configuration`
- Check for required API tokens and warn if missing
- Initialize WebSocket client if connection_manager is available

### Service Pattern
```python
class MyService:
    def __init__(self, db_session: Optional[Session] = None, 
                 workflow_id: Optional[str] = None,
                 connection_manager = None):
        log_service_configuration(logger, "MyService", event="init", 
                                 details={"has_db": bool(db_session)})
        
        self.db_session = db_session
        self.workflow_id = workflow_id
        
        # WebSocket client for progress updates
        if connection_manager and workflow_id:
            from src.services.websocket_client import WebSocketNotificationService
            self.ws_client = WebSocketNotificationService(connection_manager)
        else:
            self.ws_client = None
```

### Error Handling
- Use `UserFriendlyError` for user-facing errors
- Include `technical_details` and `action_required` fields
- Implement proper error classification with `ErrorClassifier`
- Log errors with structured logging
- Provide helpful error messages for API configuration issues

### Error Pattern
```python
from src.utils.error_messages import UserFriendlyError

raise UserFriendlyError(
    message="Failed to generate survey",
    technical_details="Replicate API returned 500",
    action_required="Check your REPLICATE_API_TOKEN configuration"
)
```

## API Router Patterns

### Router Organization
- 16 API routers in `src/api/` directory
- Router naming convention: `{domain}_router` (e.g., `rfq_router`, `survey_router`)
- All routers use `/api/v1` prefix
- Use FastAPI's `APIRouter` for modular organization
- Include routers in `src/main.py` with proper prefix

### Router Pattern
```python
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from src.database.connection import get_db

router = APIRouter(prefix="/api/v1/{domain}", tags=["{domain}"])

@router.post("/")
async def create_resource(data: ResourceRequest, db: Session = Depends(get_db)):
    # Implementation in service class
    service = ResourceService(db)
    return await service.create(data)
```

## Document Processing Patterns

### .docx Parsing
- Advanced .docx parsing with field extraction in `src/services/document_parser.py`
- RFQ field mapping and analysis with `DocumentAnalysisResponse`
- Document upload and processing workflow
- Cancellation support for document processing
- Handle structured content extraction

### Document Processing Flow
1. Upload document via `/api/v1/rfq/upload-document`
2. Parse with `DocumentParser.parse_document()`
3. Extract fields with `FieldExtractionService`
4. Map to RFQ structure
5. Support cancellation via session_id

## Export System Patterns

### Export Architecture
- Structured export in `src/services/export/`
- Base exporter class with renderer pattern
- DOCX export with proper formatting via `docx_renderer.py`
- Survey preview before export
- Include all survey metadata and structure

### Export Pattern
```python
from src.services.export.base import BaseExporter
from src.services.export.docx_renderer import DocxRenderer

exporter = BaseExporter(renderer=DocxRenderer())
file_path = await exporter.export(survey_data, output_path)
```

## Admin & Operations Patterns

### Admin API
- Admin endpoints in `src/api/admin.py`
- Database migration endpoints: `/api/v1/admin/migrate-all`
- Health check endpoints: `/api/v1/admin/health`
- Migration status: `/api/v1/admin/check-migration-status`
- Settings management via API in `src/api/settings.py`

### LLM Audit System
- Track all AI calls in `llm_audit` table
- Monitor token usage, latency, costs
- Store raw responses for debugging
- Accessible via `/api/v1/llm-audit` endpoints

## File Organization
- **Backend code**: `src/` directory
  - `src/api/` - FastAPI routers
  - `src/services/` - Business logic (24 service classes)
  - `src/workflows/` - LangGraph workflow nodes
  - `src/database/` - Models and connection
  - `src/config/` - Settings and configuration
  - `src/utils/` - Utility functions
- **Frontend code**: `frontend/src/` directory
- **Tests**: `tests/` directory (unit, integration, evaluation)
- **Documentation**: `docs/` directory
- **Database migrations**: `migrations/` directory (SQL files)
- **Evaluation framework**: `evaluations/` directory

## Testing

### Test Organization
- **Unit tests**: `tests/unit/` - Test individual service classes
- **Integration tests**: `tests/integration/` - Test API endpoints and workflows
- **Evaluation tests**: `tests/evaluation/` - Test evaluation framework
- Use pytest for Python testing
- Use Jest for frontend testing
- Maintain test coverage above 80%

### Regression Testing
- **Critical**: When encountering regressions or bugs, immediately augment the test suite
- Add specific test cases that reproduce the regression
- Include edge cases and boundary conditions
- Add integration tests for API endpoints that failed
- Update existing tests if they were insufficient
- Document the regression scenario in test comments
- Use `scripts/run_tests.sh` for comprehensive test execution
- Run tests before any deployment to catch regressions early

### Test Execution
```bash
# Run critical tests (must pass for deployment)
./scripts/run_tests.sh critical --quiet

# Run integration tests
./scripts/run_tests.sh integration

# Run all tests
./scripts/run_tests.sh all
```

## Docker & Deployment

### Docker Patterns
- Use multi-stage Docker builds for optimization
- Follow docker-compose patterns for local development
- Use environment variables for configuration
- CORS is wide-open for development (`allow_origins=["*"]`) - tighten for production

### Deployment Scripts
- **Production**: Use `deploy.sh` for Railway deployment
- **Local Development**: Use `start-local.sh` for local environment setup
- Always run tests before production deployment
- Verify environment variables are properly configured

### Railway Deployment
- Follow the patterns in `deploy.sh`
- Use migration endpoints in `admin.py` for database migrations
- Monitor deployment logs for any issues
- Use Railway's environment variable management
- Auto-migration available with `--auto-migrate` flag

### Local Development
- Use `start-local.sh` for consistent local setup
- Use migration endpoints in `admin.py` for database migrations
- Ensure Docker containers are properly configured
- Use local environment files for development-specific settings
- Ports: Backend 8000, Frontend 3000

## Golden Standards & RAG

### Golden Standards
- Store in `golden_rfq_survey_pairs` table
- Track quality_score and usage_count
- Use for RAG retrieval with similarity search
- Implement tiered retrieval strategy
- Update usage_count when golden example is used

### Retrieval Strategy
1. **Tier 1**: Exact golden RFQ-survey pairs (semantic similarity + methodology match)
2. **Tier 2**: Methodology blocks extracted from golden surveys
3. **Tier 3**: Individual template questions (fallback)

## Code Review Guidelines
- All code changes must include tests
- Follow the existing error handling patterns
- Use proper logging with structured logging format
- Ensure database migrations are backward compatible
- Follow security best practices for API endpoints
- Update documentation when adding features
- Run linting and type checking before committing
- **Frontend Changes**: Always run `cd frontend && npm run build` after making frontend changes to verify TypeScript compilation and catch build errors before committing

## Meta-Rule: Regression Documentation
- **When regressions are discovered, document the learnings and prevention strategies in cursor rules**
- This creates a self-improving documentation system that captures institutional knowledge
- **Example**: The async/await regression (2024-10-25) was documented here to prevent future occurrences
- **Process**:
  1. Identify the root cause of the regression
  2. Document the specific error patterns and symptoms
  3. Add prevention strategies to cursor rules
  4. Update testing guidelines to catch similar issues
  5. Add static analysis rules where possible

## Dependencies Management
- Use `pyproject.toml` for Python dependencies
- Use `package.json` for frontend dependencies
- Keep dependencies up to date and secure
- Use `uv` for Python package management (fast and reliable)
- Run `uv sync` to install dependencies
- Run `uv lock` after dependency changes

## Documentation
- Update relevant documentation when making changes
- Follow the existing documentation patterns in `docs/`
- Include docstrings for all public functions and classes
- Update README files when adding new features
- Document API endpoints with FastAPI's built-in docs

## Performance Optimization
- Optimize database queries (use indexes, avoid N+1)
- Use proper caching strategies with Redis (graceful fallback if unavailable)
- Use pgvector indexes for vector similarity search
- Follow the existing optimization patterns
- Monitor and profile performance-critical code
- Preload ML models at startup (not per-request)

## Security Best Practices
- Validate all input data with Pydantic models
- Use proper authentication and authorization (implement as needed)
- Follow OWASP security guidelines
- Keep dependencies updated for security patches
- Never log sensitive data (API tokens, passwords)
- Use environment variables for all secrets
- Implement rate limiting for public endpoints

## Key Success Metrics
- **Generation Time**: <30 seconds end-to-end
- **Golden Similarity**: >0.75 similarity score
- **Cleanup Time**: <30 minutes (vs 3-4 hours baseline)
- **Methodology Compliance**: 80%+ validation pass rate
- **Test Coverage**: >80% code coverage

## Common Patterns to Follow

### Service Method Pattern
```python
async def process_item(self, item_id: str) -> Dict[str, Any]:
    """Process an item with progress tracking"""
    logger.info(f"Processing item: {item_id}")
    
    # Send progress update if WebSocket available
    if self.ws_client and self.workflow_id:
        await self.ws_client.send_progress_update(
            self.workflow_id, 
            {"type": "progress", "message": "Processing..."}
        )
    
    # Business logic
    result = await self._do_processing(item_id)
    
    logger.info(f"Completed processing: {item_id}")
    return result
```

### Database Query Pattern
```python
from sqlalchemy import select
from sqlalchemy.orm import Session

async def get_items(db: Session, filters: Dict[str, Any]) -> List[Model]:
    """Query with proper async pattern"""
    stmt = select(Model).where(Model.status == filters.get("status"))
    result = await db.execute(stmt)
    return result.scalars().all()
```

### API Endpoint Pattern
```python
@router.post("/items", response_model=ItemResponse)
async def create_item(
    request: ItemRequest,
    db: Session = Depends(get_db)
) -> ItemResponse:
    """Create a new item with proper error handling"""
    try:
        service = ItemService(db)
        result = await service.create_item(request)
        return ItemResponse(**result)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Failed to create item: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")
```

## Notes
- FAISS is available in dependencies for local dev/testing, but pgvector is used in production
- Redis is optional with graceful fallback to in-memory cache
- WebSocket connections are optional - services should work without them
- All services should handle missing dependencies gracefully
- The codebase uses structured logging - follow the existing format