"""
API endpoints for survey annotations
"""

from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_
from typing import List, Optional, Dict, Any, Union
from pydantic import BaseModel, Field, field_validator
from datetime import datetime
import logging

from src.database.connection import get_db
from src.database.models import QuestionAnnotation, SectionAnnotation, SurveyAnnotation
from src.services.advanced_labeling_service import AdvancedLabelingService

logger = logging.getLogger(__name__)

router = APIRouter()

# Pydantic models for API

class QuestionAnnotationRequest(BaseModel):
    question_id: str
    required: bool = True
    quality: int = Field(ge=1, le=5)
    relevant: int = Field(ge=1, le=5)
    methodological_rigor: int = Field(ge=1, le=5)
    content_validity: int = Field(ge=1, le=5)
    respondent_experience: int = Field(ge=1, le=5)
    analytical_value: int = Field(ge=1, le=5)
    business_impact: int = Field(ge=1, le=5)
    comment: Optional[str] = None
    labels: Optional[List[str]] = None
    annotator_id: str = "current-user"
    ai_confidence: Optional[float] = Field(ge=0.0, le=1.0, default=None)

class SectionAnnotationRequest(BaseModel):
    section_id: int
    quality: int = Field(ge=1, le=5)
    relevant: int = Field(ge=1, le=5)
    methodological_rigor: int = Field(ge=1, le=5)
    content_validity: int = Field(ge=1, le=5)
    respondent_experience: int = Field(ge=1, le=5)
    analytical_value: int = Field(ge=1, le=5)
    business_impact: int = Field(ge=1, le=5)
    comment: Optional[str] = None
    labels: Optional[List[str]] = None
    annotator_id: str = "current-user"

    # Advanced labeling fields
    section_classification: Optional[str] = None
    mandatory_elements: Optional[Dict[str, Any]] = None
    compliance_score: Optional[int] = None

class SurveyLevelAnnotationRequest(BaseModel):
    survey_id: str
    overall_comment: Optional[str] = None
    labels: Optional[List[str]] = None
    annotator_id: str = "current-user"

    # Advanced labeling fields
    detected_labels: Optional[Dict[str, Any]] = None
    compliance_report: Optional[Dict[str, Any]] = None
    advanced_metadata: Optional[Dict[str, Any]] = None

    # Survey-level quality ratings
    overall_quality: Optional[int] = Field(ge=1, le=5, default=None)
    survey_relevance: Optional[int] = Field(ge=1, le=5, default=None)
    methodology_score: Optional[int] = Field(ge=1, le=5, default=None)
    respondent_experience_score: Optional[int] = Field(ge=1, le=5, default=None)
    business_value_score: Optional[int] = Field(ge=1, le=5, default=None)

    # Survey classification
    survey_type: Optional[str] = None
    industry_category: Optional[str] = None
    research_methodology: Optional[List[str]] = None
    target_audience: Optional[str] = None
    survey_complexity: Optional[str] = None
    estimated_duration: Optional[int] = None
    compliance_status: Optional[str] = None

class BulkAnnotationRequest(BaseModel):
    survey_id: str
    question_annotations: List[QuestionAnnotationRequest] = []
    section_annotations: List[SectionAnnotationRequest] = []
    overall_comment: Optional[str] = None
    annotator_id: str = "current-user"

    # Advanced labeling fields for survey-level
    detected_labels: Optional[Dict[str, Any]] = None
    compliance_report: Optional[Dict[str, Any]] = None
    advanced_metadata: Optional[Dict[str, Any]] = None

class QuestionAnnotationResponse(BaseModel):
    id: int
    question_id: str
    survey_id: str
    required: bool
    quality: int
    relevant: int
    methodological_rigor: int
    content_validity: int
    respondent_experience: int
    analytical_value: int
    business_impact: int
    comment: Optional[str]
    labels: Optional[Any] = None
    annotator_id: str
    created_at: datetime
    updated_at: Optional[datetime]
    
    # AI annotation fields
    ai_generated: Optional[bool] = None
    ai_confidence: Optional[float] = None
    human_verified: Optional[bool] = None
    generation_timestamp: Optional[datetime] = None
    
    # Human override tracking fields
    human_overridden: Optional[bool] = None
    override_timestamp: Optional[datetime] = None
    original_ai_quality: Optional[int] = None
    original_ai_relevant: Optional[int] = None
    original_ai_comment: Optional[str] = None

class SectionAnnotationResponse(BaseModel):
    id: int
    section_id: int
    survey_id: str
    quality: int
    relevant: int
    methodological_rigor: int
    content_validity: int
    respondent_experience: int
    analytical_value: int
    business_impact: int
    comment: Optional[str]
    labels: Optional[Any] = None
    annotator_id: str
    created_at: datetime
    updated_at: Optional[datetime]

    # Advanced labeling fields
    section_classification: Optional[str] = None
    mandatory_elements: Optional[Dict[str, Any]] = None
    compliance_score: Optional[int] = None
    
    # AI annotation fields
    ai_generated: Optional[bool] = None
    ai_confidence: Optional[float] = None
    human_verified: Optional[bool] = None
    generation_timestamp: Optional[datetime] = None
    
    # Human override tracking fields
    human_overridden: Optional[bool] = None
    override_timestamp: Optional[datetime] = None
    original_ai_quality: Optional[int] = None
    original_ai_relevant: Optional[int] = None
    original_ai_comment: Optional[str] = None

class SurveyLevelAnnotationResponse(BaseModel):
    id: int
    survey_id: str
    overall_comment: Optional[str] = None
    labels: Optional[List[str]] = None
    annotator_id: Optional[str] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None

    # Advanced labeling fields
    detected_labels: Optional[Dict[str, Any]] = None
    compliance_report: Optional[Dict[str, Any]] = None
    advanced_metadata: Optional[Dict[str, Any]] = None

    # Survey-level quality ratings
    overall_quality: Optional[int] = None
    survey_relevance: Optional[int] = None
    methodology_score: Optional[int] = None
    respondent_experience_score: Optional[int] = None
    business_value_score: Optional[int] = None

    # Survey classification
    survey_type: Optional[str] = None
    industry_category: Optional[str] = None
    research_methodology: Optional[List[str]] = None
    target_audience: Optional[str] = None
    survey_complexity: Optional[str] = None
    estimated_duration: Optional[int] = None
    compliance_status: Optional[str] = None

    class Config:
        from_attributes = True

class SurveyAnnotationsResponse(BaseModel):
    survey_id: str
    question_annotations: List[QuestionAnnotationResponse]
    section_annotations: List[SectionAnnotationResponse]
    overall_comment: Optional[str]
    annotator_id: str
    created_at: datetime
    updated_at: Optional[datetime]

    # Advanced labeling fields
    detected_labels: Optional[Dict[str, Any]] = None
    compliance_report: Optional[Dict[str, Any]] = None
    advanced_metadata: Optional[Dict[str, Any]] = None

# API Endpoints

@router.get("/surveys/{survey_id}/annotations", response_model=SurveyAnnotationsResponse)
def get_survey_annotations(
    survey_id: str,
    annotator_id: Optional[str] = "current-user",
    include_ai_annotations: bool = True,
    db: Session = Depends(get_db)
):
    """Get all annotations for a survey, optionally including AI annotations"""
    
    # Build query conditions
    question_conditions = [QuestionAnnotation.survey_id == survey_id]
    section_conditions = [SectionAnnotation.survey_id == survey_id]
    
    if include_ai_annotations:
        # Include both user and AI annotations (including DOCX parser annotations)
        question_conditions.append(
            or_(
                QuestionAnnotation.annotator_id == annotator_id,
                QuestionAnnotation.annotator_id == "ai_system",
                QuestionAnnotation.ai_generated == True  # Include any AI-generated annotations regardless of annotator_id
            )
        )
        section_conditions.append(
            or_(
                SectionAnnotation.annotator_id == annotator_id,
                SectionAnnotation.annotator_id == "ai_system",
                SectionAnnotation.ai_generated == True  # Include any AI-generated annotations regardless of annotator_id
            )
        )
    else:
        # Only include user annotations
        question_conditions.append(QuestionAnnotation.annotator_id == annotator_id)
        section_conditions.append(SectionAnnotation.annotator_id == annotator_id)
    
    # Get question annotations
    question_annotations = db.query(QuestionAnnotation).filter(
        and_(*question_conditions)
    ).all()
    
    # Get section annotations
    section_annotations = db.query(SectionAnnotation).filter(
        and_(*section_conditions)
    ).all()
    
    # Get survey-level annotation
    survey_annotation = db.query(SurveyAnnotation).filter(
        and_(
            SurveyAnnotation.survey_id == survey_id,
            SurveyAnnotation.annotator_id == annotator_id
        )
    ).first()
    
    # Return empty response instead of 404 when no annotations exist
    if not question_annotations and not section_annotations and not survey_annotation:
        return SurveyAnnotationsResponse(
            survey_id=survey_id,
            question_annotations=[],
            section_annotations=[],
            overall_comment=None,
            annotator_id=annotator_id,
            created_at=datetime.now(),
            updated_at=datetime.now()
        )
    
    # Get creation/update timestamps
    all_annotations = question_annotations + section_annotations
    if all_annotations:
        created_at = min(ann.created_at for ann in all_annotations)
        updated_at = max(ann.updated_at or ann.created_at for ann in all_annotations)
    else:
        created_at = datetime.now()
        updated_at = datetime.now()
    
    return SurveyAnnotationsResponse(
        survey_id=survey_id,
        question_annotations=[
            QuestionAnnotationResponse(
                id=qa.id,
                question_id=qa.question_id,
                survey_id=qa.survey_id,
                required=qa.required,
                quality=qa.quality,
                relevant=qa.relevant,
                methodological_rigor=qa.methodological_rigor,
                content_validity=qa.content_validity,
                respondent_experience=qa.respondent_experience,
                analytical_value=qa.analytical_value,
                business_impact=qa.business_impact,
                comment=qa.comment,
                labels=qa.labels,
                annotator_id=qa.annotator_id,
                created_at=qa.created_at,
                updated_at=qa.updated_at,
                # AI annotation fields
                ai_generated=qa.ai_generated,
                ai_confidence=float(qa.ai_confidence) if qa.ai_confidence else None,
                human_verified=qa.human_verified,
                generation_timestamp=qa.generation_timestamp,
                # Human override tracking fields
                human_overridden=getattr(qa, 'human_overridden', None),
                override_timestamp=getattr(qa, 'override_timestamp', None),
                original_ai_quality=getattr(qa, 'original_ai_quality', None),
                original_ai_relevant=getattr(qa, 'original_ai_relevant', None),
                original_ai_comment=getattr(qa, 'original_ai_comment', None)
            ) for qa in question_annotations
        ],
        section_annotations=[
            SectionAnnotationResponse(
                id=sa.id,
                section_id=sa.section_id,
                survey_id=sa.survey_id,
                quality=sa.quality,
                relevant=sa.relevant,
                methodological_rigor=sa.methodological_rigor,
                content_validity=sa.content_validity,
                respondent_experience=sa.respondent_experience,
                analytical_value=sa.analytical_value,
                business_impact=sa.business_impact,
                comment=sa.comment,
                labels=sa.labels,
                annotator_id=sa.annotator_id,
                created_at=sa.created_at,
                updated_at=sa.updated_at,
                # Advanced labeling fields (only if they exist)
                section_classification=getattr(sa, 'section_classification', None),
                mandatory_elements=getattr(sa, 'mandatory_elements', None),
                compliance_score=getattr(sa, 'compliance_score', None),
                # AI annotation fields
                ai_generated=sa.ai_generated,
                ai_confidence=float(sa.ai_confidence) if sa.ai_confidence else None,
                human_verified=sa.human_verified,
                generation_timestamp=sa.generation_timestamp,
                # Human override tracking fields
                human_overridden=getattr(sa, 'human_overridden', None),
                override_timestamp=getattr(sa, 'override_timestamp', None),
                original_ai_quality=getattr(sa, 'original_ai_quality', None),
                original_ai_relevant=getattr(sa, 'original_ai_relevant', None),
                original_ai_comment=getattr(sa, 'original_ai_comment', None)
            ) for sa in section_annotations
        ],
        overall_comment=survey_annotation.overall_comment if survey_annotation else None,
        annotator_id=annotator_id,
        created_at=created_at,
        updated_at=updated_at,
        # Advanced labeling fields
        detected_labels=survey_annotation.detected_labels if survey_annotation else None,
        compliance_report=survey_annotation.compliance_report if survey_annotation else None,
        advanced_metadata=survey_annotation.advanced_metadata if survey_annotation else None
    )

@router.post("/annotations/survey/{survey_id}/bulk")
def save_bulk_annotations(
    survey_id: str,
    annotations: BulkAnnotationRequest,
    db: Session = Depends(get_db)
):
    """Save multiple annotations for a survey in bulk"""
    
    logger.info(f"🔍 [API] Starting bulk annotation save for survey_id={survey_id}")
    logger.info(f"📊 [API] Processing {len(annotations.question_annotations)} question annotations and {len(annotations.section_annotations)} section annotations")
    
    try:
        # Process question annotations
        logger.info("📝 [API] Processing question annotations...")
        for i, qa_req in enumerate(annotations.question_annotations):
            logger.debug(f"🔍 [API] Processing question annotation {i+1}/{len(annotations.question_annotations)}: question_id={qa_req.question_id}")
            
            # Check if annotation already exists
            existing_qa = db.query(QuestionAnnotation).filter(
                and_(
                    QuestionAnnotation.question_id == qa_req.question_id,
                    QuestionAnnotation.annotator_id == qa_req.annotator_id,
                    QuestionAnnotation.survey_id == survey_id
                )
            ).first()
            
            if existing_qa:
                logger.debug(f"🔄 [API] Updating existing question annotation for question_id={qa_req.question_id}")
                
                # Check if this is a human override of an AI annotation
                is_human_override = (
                    existing_qa.ai_generated and 
                    existing_qa.annotator_id == "ai_system" and 
                    qa_req.annotator_id == "current-user"
                )
                
                if is_human_override:
                    logger.info(f"👤 [API] Human overriding AI annotation for question_id={qa_req.question_id}")
                    # Store original AI values before updating
                    existing_qa.original_ai_quality = existing_qa.quality
                    existing_qa.original_ai_relevant = existing_qa.relevant
                    existing_qa.original_ai_comment = existing_qa.comment
                    existing_qa.human_overridden = True
                    existing_qa.override_timestamp = datetime.now()
                    existing_qa.human_verified = True  # Mark as human-verified since they're overriding
                
                # Update existing annotation
                existing_qa.required = qa_req.required
                existing_qa.quality = qa_req.quality
                existing_qa.relevant = qa_req.relevant
                existing_qa.methodological_rigor = qa_req.methodological_rigor
                existing_qa.content_validity = qa_req.content_validity
                existing_qa.respondent_experience = qa_req.respondent_experience
                existing_qa.analytical_value = qa_req.analytical_value
                existing_qa.business_impact = qa_req.business_impact
                existing_qa.comment = qa_req.comment
                existing_qa.labels = qa_req.labels
                existing_qa.updated_at = datetime.now()
            else:
                logger.debug(f"➕ [API] Creating new question annotation for question_id={qa_req.question_id}")
                
                # Determine if this is an AI-generated annotation
                is_ai_generated = qa_req.annotator_id == "ai_system"
                
                # Create new annotation
                new_qa = QuestionAnnotation(
                    question_id=qa_req.question_id,
                    survey_id=survey_id,
                    required=qa_req.required,
                    quality=qa_req.quality,
                    relevant=qa_req.relevant,
                    methodological_rigor=qa_req.methodological_rigor,
                    content_validity=qa_req.content_validity,
                    respondent_experience=qa_req.respondent_experience,
                    analytical_value=qa_req.analytical_value,
                    business_impact=qa_req.business_impact,
                    comment=qa_req.comment,
                    labels=qa_req.labels,
                    annotator_id=qa_req.annotator_id,
                    # Set AI-generated fields based on annotator_id
                    ai_generated=is_ai_generated,
                    ai_confidence=qa_req.ai_confidence if is_ai_generated else None,
                    human_verified=False if is_ai_generated else True,
                    generation_timestamp=datetime.now() if is_ai_generated else None,
                    # Set human override tracking fields
                    human_overridden=False
                )
                db.add(new_qa)
        
        logger.info("✅ [API] Completed processing question annotations")
        
        # Process section annotations
        logger.info("📝 [API] Processing section annotations...")
        for i, sa_req in enumerate(annotations.section_annotations):
            logger.debug(f"🔍 [API] Processing section annotation {i+1}/{len(annotations.section_annotations)}: section_id={sa_req.section_id}")
            
            # Check if annotation already exists
            existing_sa = db.query(SectionAnnotation).filter(
                and_(
                    SectionAnnotation.section_id == sa_req.section_id,
                    SectionAnnotation.annotator_id == sa_req.annotator_id,
                    SectionAnnotation.survey_id == survey_id
                )
            ).first()
            
            if existing_sa:
                logger.debug(f"🔄 [API] Updating existing section annotation for section_id={sa_req.section_id}")
                
                # Check if this is a human override of an AI annotation
                is_human_override = (
                    existing_sa.ai_generated and 
                    existing_sa.annotator_id == "ai_system" and 
                    sa_req.annotator_id == "current-user"
                )
                
                if is_human_override:
                    logger.info(f"👤 [API] Human overriding AI annotation for section_id={sa_req.section_id}")
                    # Store original AI values before updating
                    existing_sa.original_ai_quality = existing_sa.quality
                    existing_sa.original_ai_relevant = existing_sa.relevant
                    existing_sa.original_ai_comment = existing_sa.comment
                    existing_sa.human_overridden = True
                    existing_sa.override_timestamp = datetime.now()
                    existing_sa.human_verified = True  # Mark as human-verified since they're overriding
                
                # Update existing annotation
                existing_sa.quality = sa_req.quality
                existing_sa.relevant = sa_req.relevant
                existing_sa.methodological_rigor = sa_req.methodological_rigor
                existing_sa.content_validity = sa_req.content_validity
                existing_sa.respondent_experience = sa_req.respondent_experience
                existing_sa.analytical_value = sa_req.analytical_value
                existing_sa.business_impact = sa_req.business_impact
                existing_sa.comment = sa_req.comment
                existing_sa.labels = sa_req.labels
                existing_sa.updated_at = datetime.now()
                # Update advanced labeling fields
                existing_sa.section_classification = sa_req.section_classification
                existing_sa.mandatory_elements = sa_req.mandatory_elements
                existing_sa.compliance_score = sa_req.compliance_score
            else:
                logger.debug(f"➕ [API] Creating new section annotation for section_id={sa_req.section_id}")
                # Create new annotation
                new_sa = SectionAnnotation(
                    section_id=sa_req.section_id,
                    survey_id=survey_id,
                    quality=sa_req.quality,
                    relevant=sa_req.relevant,
                    methodological_rigor=sa_req.methodological_rigor,
                    content_validity=sa_req.content_validity,
                    respondent_experience=sa_req.respondent_experience,
                    analytical_value=sa_req.analytical_value,
                    business_impact=sa_req.business_impact,
                    comment=sa_req.comment,
                    labels=sa_req.labels,
                    annotator_id=sa_req.annotator_id,
                    # Advanced labeling fields
                    section_classification=sa_req.section_classification,
                    mandatory_elements=sa_req.mandatory_elements,
                    compliance_score=sa_req.compliance_score
                )
                db.add(new_sa)
        
        logger.info("✅ [API] Completed processing section annotations")
        
        # Process survey-level annotation
        if annotations.overall_comment or annotations.detected_labels or annotations.compliance_report or annotations.advanced_metadata:
            logger.info("📝 [API] Processing survey-level annotation...")
            existing_survey_ann = db.query(SurveyAnnotation).filter(
                and_(
                    SurveyAnnotation.survey_id == survey_id,
                    SurveyAnnotation.annotator_id == annotations.annotator_id
                )
            ).first()

            if existing_survey_ann:
                logger.debug(f"🔄 [API] Updating existing survey annotation for survey_id={survey_id}")
                existing_survey_ann.overall_comment = annotations.overall_comment
                existing_survey_ann.updated_at = datetime.now()
                # Update advanced labeling fields
                existing_survey_ann.detected_labels = annotations.detected_labels
                existing_survey_ann.compliance_report = annotations.compliance_report
                existing_survey_ann.advanced_metadata = annotations.advanced_metadata
            else:
                logger.debug(f"➕ [API] Creating new survey annotation for survey_id={survey_id}")
                new_survey_ann = SurveyAnnotation(
                    survey_id=survey_id,
                    overall_comment=annotations.overall_comment,
                    annotator_id=annotations.annotator_id,
                    # Advanced labeling fields
                    detected_labels=annotations.detected_labels,
                    compliance_report=annotations.compliance_report,
                    advanced_metadata=annotations.advanced_metadata
                )
                db.add(new_survey_ann)
            logger.info("✅ [API] Completed processing survey-level annotation")
        
        # Commit all changes
        logger.info("💾 [API] Committing all changes to database...")
        db.commit()
        logger.info("✅ [API] Successfully committed all changes")
        
        logger.info(f"🎉 [API] Bulk annotation save completed successfully for survey_id={survey_id}")
        return {"message": "Annotations saved successfully", "survey_id": survey_id}
        
    except Exception as e:
        logger.error(f"❌ [API] Failed to save bulk annotations for survey_id={survey_id}: {str(e)}", exc_info=True)
        db.rollback()
        logger.error(f"🔄 [API] Database transaction rolled back due to error")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to save annotations: {str(e)}"
        )

@router.get("/annotations/stats")
def get_annotation_stats(
    annotator_id: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """Get annotation statistics"""
    
    # Base queries
    question_query = db.query(QuestionAnnotation)
    section_query = db.query(SectionAnnotation)
    
    # Filter by annotator if specified
    if annotator_id:
        question_query = question_query.filter(QuestionAnnotation.annotator_id == annotator_id)
        section_query = section_query.filter(SectionAnnotation.annotator_id == annotator_id)
    
    # Count annotations
    question_count = question_query.count()
    section_count = section_query.count()
    
    # Get average scores
    question_annotations = question_query.all()
    section_annotations = section_query.all()
    
    if question_annotations:
        avg_question_quality = sum(qa.quality for qa in question_annotations) / len(question_annotations)
        avg_question_relevant = sum(qa.relevant for qa in question_annotations) / len(question_annotations)
        avg_question_pillars = {
            'methodological_rigor': sum(qa.methodological_rigor for qa in question_annotations) / len(question_annotations),
            'content_validity': sum(qa.content_validity for qa in question_annotations) / len(question_annotations),
            'respondent_experience': sum(qa.respondent_experience for qa in question_annotations) / len(question_annotations),
            'analytical_value': sum(qa.analytical_value for qa in question_annotations) / len(question_annotations),
            'business_impact': sum(qa.business_impact for qa in question_annotations) / len(question_annotations),
        }
    else:
        avg_question_quality = 0
        avg_question_relevant = 0
        avg_question_pillars = {}
    
    if section_annotations:
        avg_section_quality = sum(sa.quality for sa in section_annotations) / len(section_annotations)
        avg_section_relevant = sum(sa.relevant for sa in section_annotations) / len(section_annotations)
        avg_section_pillars = {
            'methodological_rigor': sum(sa.methodological_rigor for sa in section_annotations) / len(section_annotations),
            'content_validity': sum(sa.content_validity for sa in section_annotations) / len(section_annotations),
            'respondent_experience': sum(sa.respondent_experience for sa in section_annotations) / len(section_annotations),
            'analytical_value': sum(sa.analytical_value for sa in section_annotations) / len(section_annotations),
            'business_impact': sum(sa.business_impact for sa in section_annotations) / len(section_annotations),
        }
    else:
        avg_section_quality = 0
        avg_section_relevant = 0
        avg_section_pillars = {}
    
    return {
        "question_annotations": {
            "count": question_count,
            "avg_quality": round(avg_question_quality, 2),
            "avg_relevant": round(avg_question_relevant, 2),
            "avg_pillars": {k: round(v, 2) for k, v in avg_question_pillars.items()}
        },
        "section_annotations": {
            "count": section_count,
            "avg_quality": round(avg_section_quality, 2),
            "avg_relevant": round(avg_section_relevant, 2),
            "avg_pillars": {k: round(v, 2) for k, v in avg_section_pillars.items()}
        },
        "total_annotations": question_count + section_count
    }

@router.post("/annotations/survey/{survey_id}/advanced-labeling")
def apply_advanced_labeling(
    survey_id: str,
    db: Session = Depends(get_db)
):
    """Apply advanced labeling to all annotations for a survey"""

    try:
        # Initialize advanced labeling service
        labeling_service = AdvancedLabelingService(db)

        # Apply bulk labeling
        results = labeling_service.apply_bulk_labeling(survey_id)

        return {
            "message": "Advanced labeling applied successfully",
            "results": results
        }

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to apply advanced labeling: {str(e)}"
        )

@router.get("/annotations/survey/{survey_id}/compliance-report")
def get_compliance_report(
    survey_id: str,
    db: Session = Depends(get_db)
):
    """Get compliance report for a survey"""

    try:
        # Get survey annotation
        survey_annotation = db.query(SurveyAnnotation).filter(
            SurveyAnnotation.survey_id == survey_id
        ).first()

        if not survey_annotation or not survey_annotation.compliance_report:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Compliance report not found. Run advanced labeling first."
            )

        return {
            "survey_id": survey_id,
            "compliance_report": survey_annotation.compliance_report,
            "advanced_metadata": survey_annotation.advanced_metadata
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get compliance report: {str(e)}"
        )

@router.get("/annotations/survey/{survey_id}/detected-labels")
def get_detected_labels(
    survey_id: str,
    db: Session = Depends(get_db)
):
    """Get detected labels for a survey"""

    try:
        # Get survey annotation
        survey_annotation = db.query(SurveyAnnotation).filter(
            SurveyAnnotation.survey_id == survey_id
        ).first()

        if not survey_annotation or not survey_annotation.detected_labels:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Detected labels not found. Run advanced labeling first."
            )

        return {
            "survey_id": survey_id,
            "detected_labels": survey_annotation.detected_labels
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get detected labels: {str(e)}"
        )

# Survey-Level Annotation Endpoints

@router.post("/surveys/{survey_id}/survey-annotation", response_model=SurveyLevelAnnotationResponse)
def create_or_update_survey_annotation(
    survey_id: str,
    annotation: SurveyLevelAnnotationRequest,
    db: Session = Depends(get_db)
):
    """Create or update survey-level annotation"""
    
    try:
        # Check if survey annotation already exists
        existing_annotation = db.query(SurveyAnnotation).filter(
            SurveyAnnotation.survey_id == survey_id
        ).first()
        
        if existing_annotation:
            # Update existing annotation
            existing_annotation.overall_comment = annotation.overall_comment
            existing_annotation.annotator_id = annotation.annotator_id
            existing_annotation.detected_labels = annotation.detected_labels
            existing_annotation.compliance_report = annotation.compliance_report
            existing_annotation.advanced_metadata = annotation.advanced_metadata
            existing_annotation.updated_at = datetime.utcnow()
            
            db.commit()
            db.refresh(existing_annotation)
            return existing_annotation
        else:
            # Create new annotation
            new_annotation = SurveyAnnotation(
                survey_id=survey_id,
                overall_comment=annotation.overall_comment,
                annotator_id=annotation.annotator_id,
                detected_labels=annotation.detected_labels,
                compliance_report=annotation.compliance_report,
                advanced_metadata=annotation.advanced_metadata
            )
            
            db.add(new_annotation)
            db.commit()
            db.refresh(new_annotation)
            return new_annotation
            
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to save survey annotation: {str(e)}"
        )

@router.get("/surveys/{survey_id}/survey-annotation", response_model=SurveyLevelAnnotationResponse)
def get_survey_annotation(
    survey_id: str,
    db: Session = Depends(get_db)
):
    """Get survey-level annotation"""
    
    try:
        annotation = db.query(SurveyAnnotation).filter(
            SurveyAnnotation.survey_id == survey_id
        ).first()
        
        if not annotation:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Survey annotation not found"
            )
        
        return annotation
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get survey annotation: {str(e)}"
        )

@router.delete("/surveys/{survey_id}/survey-annotation")
def delete_survey_annotation(
    survey_id: str,
    db: Session = Depends(get_db)
):
    """Delete survey-level annotation"""
    
    try:
        annotation = db.query(SurveyAnnotation).filter(
            SurveyAnnotation.survey_id == survey_id
        ).first()
        
        if not annotation:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Survey annotation not found"
            )
        
        db.delete(annotation)
        db.commit()
        
        return {"message": "Survey annotation deleted successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete survey annotation: {str(e)}"
        )


@router.post("/annotations/migrate-ai-flags")
def migrate_ai_flags(db: Session = Depends(get_db)):
    """
    Migration endpoint to fix annotations with annotator_id='ai_system' but ai_generated=False
    """
    try:
        # Find all annotations with ai_system annotator but ai_generated=False
        inconsistent_annotations = db.query(QuestionAnnotation).filter(
            QuestionAnnotation.annotator_id == "ai_system",
            QuestionAnnotation.ai_generated == False
        ).all()
        
        logger.info(f"🔧 [Migration] Found {len(inconsistent_annotations)} inconsistent question annotations")
        
        # Update them to have ai_generated=True
        for annotation in inconsistent_annotations:
            annotation.ai_generated = True
            annotation.human_verified = False
            if not annotation.generation_timestamp:
                annotation.generation_timestamp = datetime.now()
        
        # Also check section annotations
        inconsistent_sections = db.query(SectionAnnotation).filter(
            SectionAnnotation.annotator_id == "ai_system",
            SectionAnnotation.ai_generated == False
        ).all()
        
        logger.info(f"🔧 [Migration] Found {len(inconsistent_sections)} inconsistent section annotations")
        
        for annotation in inconsistent_sections:
            annotation.ai_generated = True
            annotation.human_verified = False
            if not annotation.generation_timestamp:
                annotation.generation_timestamp = datetime.now()
        
        db.commit()
        
        total_fixed = len(inconsistent_annotations) + len(inconsistent_sections)
        logger.info(f"✅ [Migration] Fixed {total_fixed} inconsistent annotations")
        
        return {
            "message": f"Successfully migrated {total_fixed} annotations",
            "question_annotations_fixed": len(inconsistent_annotations),
            "section_annotations_fixed": len(inconsistent_sections)
        }
        
    except Exception as e:
        logger.error(f"❌ [Migration] Error during migration: {e}")
        db.rollback()
        raise HTTPException(status_code=500, detail=f"Migration failed: {str(e)}")