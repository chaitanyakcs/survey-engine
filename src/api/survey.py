from fastapi import APIRouter, Depends, HTTPException, WebSocket, WebSocketDisconnect
from sqlalchemy.orm import Session
from src.database import get_db, Survey
from src.database.models import LLMAudit
from src.services.survey_service import SurveyService
from src.services.survey_structure_validator import SurveyStructureValidator
from src.utils.survey_utils import get_questions_count, get_questions_and_instructions_count
from src.models.survey import QuestionUpdate, SectionUpdate, SurveySection
from pydantic import BaseModel
from typing import Optional, Dict, Any, List
from uuid import UUID
from datetime import datetime
import json
import logging

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/survey", tags=["Survey"])


class SurveyResponse(BaseModel):
    id: str
    status: str
    raw_output: Optional[Dict[Any, Any]]
    final_output: Optional[Dict[Any, Any]]
    golden_similarity_score: Optional[float]
    validation_results: Optional[Dict[str, Any]]
    edit_suggestions: Optional[Dict[str, Any]]
    pillar_scores: Optional[Dict[str, Any]]
    rfq_data: Optional[Dict[str, Any]] = None  # NEW
    rfq_id: Optional[str] = None  # NEW
    used_golden_examples: List[str] = []  # NEW
    used_golden_questions: List[str] = []  # NEW
    used_golden_sections: List[str] = []  # NEW


class EditRequest(BaseModel):
    edit_type: str
    edit_reason: str
    before_text: str
    after_text: str


class SurveyListItem(BaseModel):
    id: str
    title: str
    description: str
    status: str
    created_at: str
    methodology_tags: list
    quality_score: Optional[float]
    estimated_time: Optional[int]
    question_count: int
    instruction_count: int
    annotation: Optional[Dict[str, Any]] = None


class ValidationRequest(BaseModel):
    methodology_strict_mode: Optional[bool] = None
    golden_similarity_threshold: Optional[float] = None


class QuestionReorderRequest(BaseModel):
    question_order: List[str]


@router.get("/list", response_model=list[SurveyListItem])
async def list_surveys(
    skip: int = 0,
    limit: int = 50,
    db: Session = Depends(get_db)
):
    """Get list of all generated surveys"""
    try:
        surveys = db.query(Survey).order_by(Survey.created_at.desc()).offset(skip).limit(limit).all()
        
        survey_list = []
        for survey in surveys:
            # Extract survey data - handle both dict and JSON string formats
            survey_data = {}
            if survey.final_output:
                if isinstance(survey.final_output, dict):
                    survey_data = survey.final_output
                elif isinstance(survey.final_output, str):
                    try:
                        survey_data = json.loads(survey.final_output)
                    except json.JSONDecodeError:
                        logger.warning(f"âš ï¸ [Survey List] Failed to parse final_output JSON for survey {survey.id}")
                        survey_data = {}
            elif survey.raw_output:
                if isinstance(survey.raw_output, dict):
                    survey_data = survey.raw_output
                elif isinstance(survey.raw_output, str):
                    try:
                        survey_data = json.loads(survey.raw_output)
                    except json.JSONDecodeError:
                        logger.warning(f"âš ï¸ [Survey List] Failed to parse raw_output JSON for survey {survey.id}")
                        survey_data = {}
            
            metadata = survey_data.get('metadata', {})
            
            # Ensure methodology_tags is always a list
            methodology_tags = metadata.get('methodology_tags', [])
            if not isinstance(methodology_tags, list):
                methodology_tags = []
            
            # Use pillar scores if available, fallback to metadata quality_score
            quality_score = None
            if survey.pillar_scores and isinstance(survey.pillar_scores, dict):
                quality_score = survey.pillar_scores.get('weighted_score')
            elif metadata.get('quality_score'):
                quality_score = metadata.get('quality_score')
            
            # Extract title with improved logic for reference examples
            title = survey_data.get('title', 'Untitled Survey')
            
            # For reference examples, try to get title from golden pair if survey title is missing
            if survey.status == 'reference' and (not title or title == 'Untitled Survey'):
                try:
                    from src.database.models import GoldenRFQSurveyPair
                    golden_pair = db.query(GoldenRFQSurveyPair).filter(GoldenRFQSurveyPair.id == survey.id).first()
                    if golden_pair and golden_pair.title:
                        title = golden_pair.title
                        logger.info(f"ğŸ“ [Survey List] Using golden pair title for reference survey {survey.id}: {title}")
                except Exception as e:
                    logger.warning(f"âš ï¸ [Survey List] Failed to get golden pair title for survey {survey.id}: {e}")
            
            # Get question and instruction counts separately
            question_count, instruction_count = get_questions_and_instructions_count(survey_data)
            
            survey_list.append(SurveyListItem(
                id=str(survey.id),
                title=title,
                description=survey_data.get('description', 'No description available'),
                status=survey.status,
                created_at=survey.created_at.isoformat() if survey.created_at else '',
                methodology_tags=methodology_tags,
                quality_score=quality_score,
                estimated_time=metadata.get('estimated_time'),
                question_count=question_count,
                instruction_count=instruction_count,
                annotation=None  # Survey model doesn't have annotation field
            ))
        
        return survey_list
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list surveys: {str(e)}")


@router.get("/{survey_id}", response_model=SurveyResponse)
async def get_survey(
    survey_id: UUID,
    db: Session = Depends(get_db)
) -> SurveyResponse:
    """
    Retrieve generated survey
    Include: golden_similarity_score, validation_results, edit_suggestions
    """
    logger.info(f"ğŸ” [Survey API] Retrieving survey: survey_id={survey_id}")
    logger.info(f"ğŸ” [Survey API] Request received at: {datetime.now()}")
    
    try:
        survey_service = SurveyService(db)
        logger.info("ğŸ“‹ [Survey API] Created SurveyService, querying database")
        
        logger.info(f"ğŸ” [Survey API] About to call survey_service.get_survey({survey_id})")
        survey = survey_service.get_survey(survey_id)
        logger.info(f"ğŸ” [Survey API] get_survey returned: {survey is not None}")
        
        if not survey:
            logger.warning(f"âŒ [Survey API] Survey not found in database: survey_id={survey_id}")
            raise HTTPException(status_code=404, detail="Survey not found")
        
        # Query the related RFQ
        rfq = survey.rfq if survey else None
        logger.info(f"ğŸ” [Survey API] RFQ found: {rfq is not None}")
        logger.info(f"ğŸ” [Survey API] Survey rfq_id: {survey.rfq_id}")
        if rfq:
            logger.info(f"ğŸ” [Survey API] RFQ id: {rfq.id}")
        
        logger.info(f"âœ… [Survey API] Survey found: id={survey.id}, status={survey.status}")
        logger.info(f"ğŸ” [Survey API] Survey data keys: {list(survey.__dict__.keys()) if survey else 'None'}")
        
        logger.info(f"ğŸ” [Survey API] Getting validation results for survey_id={survey_id}")
        validation_results = survey_service.get_validation_results(survey_id)
        logger.info(f"ğŸ” [Survey API] Validation results: {validation_results is not None}")
        
        logger.info(f"ğŸ” [Survey API] Getting edit suggestions for survey_id={survey_id}")
        edit_suggestions = survey_service.get_edit_suggestions(survey_id)
        logger.info(f"ğŸ” [Survey API] Edit suggestions: {edit_suggestions is not None}")
        
        logger.info(f"ğŸ” [Survey API] Building response object...")
        # Get rfq_id from survey.rfq_id or from rfq relationship if available
        rfq_id_value = survey.rfq_id
        if not rfq_id_value and rfq:
            rfq_id_value = rfq.id
            logger.info(f"ğŸ” [Survey API] Using RFQ id from relationship: {rfq_id_value}")
        
        logger.info(f"ğŸ” [Survey API] Final rfq_id for response: {rfq_id_value}")
        
        response = SurveyResponse(
            id=str(survey.id),
            status=survey.status,  # type: ignore
            raw_output=survey.raw_output,  # type: ignore
            final_output=survey.final_output,  # type: ignore
            golden_similarity_score=float(survey.golden_similarity_score) if survey.golden_similarity_score else None,  # type: ignore
            validation_results=validation_results,
            edit_suggestions=edit_suggestions,
            pillar_scores=survey.pillar_scores,  # type: ignore
            rfq_id=str(rfq_id_value) if rfq_id_value else None,
            rfq_data=rfq.enhanced_rfq_data if rfq else None,
            used_golden_examples=[str(example_id) for example_id in survey.used_golden_examples] if survey.used_golden_examples else [],  # NEW
            used_golden_questions=[str(question_id) for question_id in survey.used_golden_questions] if survey.used_golden_questions else [],  # NEW
            used_golden_sections=[str(section_id) for section_id in survey.used_golden_sections] if survey.used_golden_sections else []  # NEW
        )
        
        logger.info(f"ğŸ‰ [Survey API] Returning survey response: status={response.status}")
        logger.info(f"ğŸ” [Survey API] Response data size: {len(str(response))} characters")
        logger.info(f"ğŸ” [Survey API] Response has final_output: {response.final_output is not None}")
        if response.final_output:
            logger.info(f"ğŸ” [Survey API] Final output keys: {list(response.final_output.keys())}")
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [Survey API] Unexpected error retrieving survey {survey_id}: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.put("/{survey_id}/edit")
async def submit_edit(
    survey_id: UUID,
    edit: EditRequest,
    db: Session = Depends(get_db)
) -> dict[str, str]:
    """
    Submit human edits
    Granularity: Based on EDIT_GRANULARITY_LEVEL setting
    Required fields: edit_type, edit_reason, before_text, after_text
    """
    try:
        survey_service = SurveyService(db)
        result = survey_service.submit_edit(
            survey_id=survey_id,
            edit_type=edit.edit_type,
            edit_reason=edit.edit_reason,
            before_text=edit.before_text,
            after_text=edit.after_text,
            annotation=edit.annotation
        )
        
        return {"status": "success", "edit_id": str(result.edit_id)}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to submit edit: {str(e)}")


@router.post("/{survey_id}/validate")
async def revalidate_survey(
    survey_id: UUID,
    validation_request: ValidationRequest,
    db: Session = Depends(get_db)
) -> dict[str, Any]:
    """
    Re-run validation with different parameters
    Parameters: methodology_strict_mode, golden_similarity_threshold
    """
    try:
        survey_service = SurveyService(db)
        result = survey_service.revalidate(
            survey_id=survey_id,
            methodology_strict_mode=validation_request.methodology_strict_mode,
            golden_similarity_threshold=validation_request.golden_similarity_threshold
        )
        
        return {"status": "success", "validation_results": result}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to revalidate: {str(e)}")


@router.delete("/{survey_id}")
async def delete_survey(
    survey_id: str,
    db: Session = Depends(get_db)
):
    """Delete a survey and clean up related records"""
    try:
        survey = db.query(Survey).filter(Survey.id == survey_id).first()
        if not survey:
            raise HTTPException(status_code=404, detail="Survey not found")
        
        # Delete related human review records first
        from src.database.models import HumanReview
        db.query(HumanReview).filter(HumanReview.survey_id == survey_id).delete()
        
        # Delete the survey
        db.delete(survey)
        db.commit()
        
        logger.info(f"ğŸ—‘ï¸ [Survey API] Survey {survey_id} and related records deleted successfully")
        return {"status": "success", "message": "Survey deleted successfully"}
        
    except Exception as e:
        db.rollback()
        logger.error(f"âŒ [Survey API] Failed to delete survey {survey_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to delete survey: {str(e)}")


@router.post("/{survey_id}/reparse")
async def reparse_survey(
    survey_id: UUID,
    db: Session = Depends(get_db)
) -> dict[str, Any]:
    """
    Reparse survey LLM output using raw response from LLM audit
    This fixes surveys generated with outdated validation rules
    """
    logger.info(f"ğŸ”„ [Survey API] Starting reparse for survey: {survey_id}")
    
    try:
        # Get the survey
        survey = db.query(Survey).filter(Survey.id == survey_id).first()
        if not survey:
            raise HTTPException(status_code=404, detail="Survey not found")
        
        # Get the raw LLM response from audit system
        from src.database.models import LLMAudit
        from src.services.generation_service import GenerationService
        
        # Find the LLM audit record for this survey
        audit_record = db.query(LLMAudit).filter(
            LLMAudit.parent_survey_id == str(survey_id),
            LLMAudit.purpose == "survey_generation"
        ).order_by(LLMAudit.created_at.desc()).first()
        
        if not audit_record:
            raise HTTPException(status_code=400, detail="No LLM audit record found for this survey")
        
        if not audit_record.raw_response:
            raise HTTPException(status_code=400, detail="No raw response found in LLM audit record")
        
        logger.info(f"ğŸ“Š [Survey API] Raw response found, starting reparse process")
        logger.info(f"ğŸ” [Survey API] Raw response length: {len(audit_record.raw_response)} characters")
        
        # Extract survey data using enhanced parsing logic
        generation_service = GenerationService(db)
        
        if isinstance(audit_record.raw_response, dict):
            # If raw_response is already parsed, extract the json_output field
            if 'json_output' in audit_record.raw_response:
                logger.info(f"ğŸ” [Survey API] Found json_output in raw_response, applying enhanced parsing")
                # Use enhanced parsing even for pre-parsed data to ensure proper structure
                json_output = audit_record.raw_response['json_output']
                if isinstance(json_output, dict):
                    # Apply enhanced validation and structure fixing
                    generation_service._validate_and_fix_survey_structure(json_output)
                    reparsed_survey = json_output
                else:
                    # If json_output is a string, parse it with enhanced logic
                    reparsed_survey = generation_service._extract_survey_json(str(json_output))
            else:
                # Fallback to the original raw_response if no json_output field
                logger.info(f"ğŸ” [Survey API] No json_output found, using raw_response directly")
                reparsed_survey = audit_record.raw_response
        else:
            # If raw_response is still a string, use the enhanced generation service
            logger.info(f"ğŸ” [Survey API] Raw response is string, using enhanced parsing")
            reparsed_survey = generation_service._extract_survey_json(audit_record.raw_response)
        
        if not reparsed_survey:
            raise HTTPException(status_code=500, detail="Failed to extract survey from raw response")
        
        logger.info(f"âœ… [Survey API] Successfully extracted survey from raw response")
        
        # Count questions before updating
        question_count = get_questions_count(reparsed_survey)
        sections_count = len(reparsed_survey.get('sections', []))
        logger.info(f"ğŸ“Š [Survey API] Reparsed survey contains {question_count} questions across {sections_count} sections")
        
        # Update the survey with reparsed data
        logger.info(f"ğŸ’¾ [Survey API] Updating survey final_output with reparsed data")
        logger.info(f"ğŸ” [Survey API] Reparsed survey keys: {list(reparsed_survey.keys())}")
        
        # Show sample of questions before saving
        if 'sections' in reparsed_survey and reparsed_survey['sections']:
            first_section = reparsed_survey['sections'][0]
            if 'questions' in first_section and first_section['questions']:
                first_question = first_section['questions'][0]
                logger.info(f"ğŸ” [Survey API] Sample question: '{first_question.get('text', 'No text')}'")
        
        # Log detailed section breakdown
        for i, section in enumerate(reparsed_survey.get('sections', [])):
            section_questions = len(section.get('questions', []))
            logger.info(f"ğŸ“‹ [Survey API] Section {i+1}: '{section.get('title', 'Unknown')}' - {section_questions} questions")
        
        survey.final_output = reparsed_survey
        survey.status = "reparsed"
        
        # Also update the raw_output to reflect the new processing
        survey.raw_output = reparsed_survey
        
        # Update the LLM audit record with the new processed output
        logger.info(f"ğŸ“ [Survey API] Updating LLM audit record with reparsed output")
        reparsed_output_json = json.dumps(reparsed_survey, indent=2)
        audit_record.output_content = reparsed_output_json
        audit_record.output_tokens = len(reparsed_output_json.split()) if reparsed_output_json else 0
        
        logger.info(f"ğŸ’¾ [Survey API] Committing changes to database")
        db.commit()
        
        logger.info(f"âœ… [Survey API] Successfully reparsed survey: {survey_id}")
        
        return {
            "status": "success",
            "message": "Survey reparsed successfully using enhanced JSON parsing",
            "survey_id": str(survey_id),
            "question_count": question_count,
            "sections_count": sections_count,
            "raw_response_length": len(str(audit_record.raw_response)),
            "reparsed_at": datetime.now().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [Survey API] Failed to reparse survey {survey_id}: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to reparse survey: {str(e)}")


@router.post("/workflow/{workflow_id}/cleanup")
async def cleanup_workflow(
    workflow_id: str,
    db: Session = Depends(get_db)
):
    """Clean up progress tracker and related resources for a workflow"""
    try:
        from src.services.progress_tracker import cleanup_progress_tracker
        
        # Clean up progress tracker
        cleanup_progress_tracker(workflow_id)
        
        logger.info(f"ğŸ§¹ [Survey API] Cleaned up resources for workflow: {workflow_id}")
        return {"status": "success", "message": "Workflow cleanup completed"}
        
    except Exception as e:
        logger.error(f"âŒ [Survey API] Failed to cleanup workflow {workflow_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to cleanup workflow: {str(e)}")


@router.put("/{survey_id}/questions/{question_id}")
async def update_question(
    survey_id: UUID,
    question_id: str,
    question_update: QuestionUpdate,
    db: Session = Depends(get_db)
):
    """Update a specific question in a survey."""
    logger.info(f"ğŸ“ [Survey API] Updating question {question_id} in survey {survey_id}")
    
    try:
        # Get the survey
        survey = db.query(Survey).filter(Survey.id == survey_id).first()
        if not survey:
            raise HTTPException(status_code=404, detail="Survey not found")
        
        # Get survey data
        survey_data = survey.final_output or survey.raw_output or {}
        if not survey_data:
            raise HTTPException(status_code=400, detail="Survey has no data to update")
        
        # Find and update the question
        question_found = False
        
        # Check if survey uses sections format
        if 'sections' in survey_data and survey_data['sections']:
            for section in survey_data['sections']:
                if 'questions' in section:
                    for i, question in enumerate(section['questions']):
                        if question.get('id') == question_id:
                            # Update the question with provided fields
                            for field, value in question_update.dict(exclude_unset=True).items():
                                question[field] = value
                            question_found = True
                            logger.info(f"âœ… [Survey API] Updated question in section {section.get('id', 'unknown')}")
                            break
                    if question_found:
                        break
        else:
            # Check flat questions format
            if 'questions' in survey_data and survey_data['questions']:
                for i, question in enumerate(survey_data['questions']):
                    if question.get('id') == question_id:
                        # Update the question with provided fields
                        for field, value in question_update.dict(exclude_unset=True).items():
                            question[field] = value
                        question_found = True
                        logger.info(f"âœ… [Survey API] Updated question in flat format")
                        break
        
        if not question_found:
            raise HTTPException(status_code=404, detail="Question not found")
        
        # Create a completely new dictionary to force SQLAlchemy to detect the change
        new_final_output = dict(survey_data)
        
        # Deep copy sections if they exist
        if 'sections' in survey_data:
            new_final_output['sections'] = []
            for section in survey_data['sections']:
                new_section = dict(section)
                if 'questions' in section:
                    new_section['questions'] = [dict(question) for question in section['questions']]
                new_final_output['sections'].append(new_section)
        
        # Deep copy questions if they exist (legacy format)
        if 'questions' in survey_data:
            new_final_output['questions'] = [dict(question) for question in survey_data['questions']]
        
        # Save the updated survey data
        logger.info(f"ğŸ’¾ [Survey API] Before save - question text: {new_final_output['sections'][0]['questions'][0]['text'] if 'sections' in new_final_output and new_final_output['sections'] else 'N/A'}")
        
        survey.final_output = new_final_output
        survey.updated_at = datetime.now()
        
        # Explicitly tell SQLAlchemy that the JSONB field has changed
        from sqlalchemy.orm.attributes import flag_modified
        flag_modified(survey, 'final_output')
        
        # Force the database to recognize the change
        db.flush()  # Flush changes to database without committing
        db.commit()
        
        logger.info(f"ğŸ’¾ [Survey API] After commit - question text: {survey.final_output['sections'][0]['questions'][0]['text'] if 'sections' in survey.final_output and survey.final_output['sections'] else 'N/A'}")
        
        # Sync to golden pair if this is a reference survey
        if survey.status == "reference":
            try:
                from src.services.golden_service import GoldenService
                golden_service = GoldenService(db)
                golden_service.sync_survey_to_golden_pair(survey.id, survey_data)
                logger.info(f"âœ… [Survey API] Synced question update to golden pair")
            except Exception as e:
                logger.warning(f"âš ï¸ [Survey API] Failed to sync question update to golden pair: {e}")
                # Don't fail the edit if sync fails
        
        logger.info(f"ğŸ’¾ [Survey API] Successfully saved updated question {question_id}")
        return {"status": "success", "message": "Question updated successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"âŒ [Survey API] Failed to update question {question_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to update question: {str(e)}")


@router.put("/{survey_id}/sections/reorder")
async def reorder_sections(
    survey_id: UUID,
    section_order: List[int],
    db: Session = Depends(get_db)
):
    """Reorder sections in a survey."""
    logger.info(f"ğŸ“ [Survey API] Reordering sections in survey {survey_id}")
    logger.info(f"ğŸ“¤ [Survey API] Received section order: {section_order}")
    
    try:
        # Get the survey
        survey = db.query(Survey).filter(Survey.id == survey_id).first()
        if not survey:
            raise HTTPException(status_code=404, detail="Survey not found")
        
        # Get survey data
        survey_data = survey.final_output or survey.raw_output or {}
        if not survey_data:
            raise HTTPException(status_code=400, detail="Survey has no data to update")
        
        logger.info(f"ğŸ“Š [Survey API] Survey data keys: {list(survey_data.keys())}")
        logger.info(f"ğŸ“Š [Survey API] Has sections key: {'sections' in survey_data}")
        logger.info(f"ğŸ“Š [Survey API] Sections count: {len(survey_data.get('sections', []))}")
        logger.info(f"ğŸ“Š [Survey API] Current sections before reorder: {[s.get('id') for s in survey_data.get('sections', [])]}")
        logger.info(f"ğŸ“Š [Survey API] Current section orders: {[s.get('order') for s in survey_data.get('sections', [])]}")
        
        # Check if we have sections to reorder
        if 'sections' not in survey_data or not survey_data['sections']:
            logger.warning(f"âš ï¸ [Survey API] No sections found in survey data!")
            logger.warning(f"âš ï¸ [Survey API] Survey data structure: {survey_data}")
            return {"status": "success", "message": "No sections to reorder"}
        
        # Reorder sections
        if 'sections' in survey_data and survey_data['sections']:
            logger.info(f"ğŸ“Š [Survey API] Found {len(survey_data['sections'])} sections to reorder")
            
            # Create a mapping of section IDs to sections
            section_map = {section.get('id'): section for section in survey_data['sections']}
            logger.info(f"ğŸ“Š [Survey API] Section map IDs: {list(section_map.keys())}")
            
            # Reorder based on the provided order
            reordered_sections = []
            for section_id in section_order:
                if section_id in section_map:
                    reordered_sections.append(section_map[section_id])
                    logger.info(f"ğŸ“Š [Survey API] Added section {section_id} to reordered list")
                else:
                    logger.warning(f"âš ï¸ [Survey API] Section ID {section_id} not found in section map!")
            
            logger.info(f"ğŸ“Š [Survey API] Reordered sections count: {len(reordered_sections)}")
            
            # Update the order property for each section to reflect the new position
            for index, section in enumerate(reordered_sections):
                section['order'] = index + 1
                logger.info(f"ğŸ“Š [Survey API] Set section {section.get('id')} order to {index + 1}")
            
            survey_data['sections'] = reordered_sections
            logger.info(f"ğŸ“Š [Survey API] Sections after reorder: {[s.get('id') for s in reordered_sections]}")
            logger.info(f"ğŸ“Š [Survey API] Section orders: {[s.get('order') for s in reordered_sections]}")
        else:
            logger.warning(f"âš ï¸ [Survey API] No sections found to reorder!")
        
        # Create a completely new dictionary to force SQLAlchemy to detect the change
        new_final_output = dict(survey_data)
        new_final_output['sections'] = [dict(section) for section in survey_data['sections']]
        
        # Save the updated survey data
        survey.final_output = new_final_output
        survey.updated_at = datetime.now()
        
        # Explicitly tell SQLAlchemy that the JSONB field has changed
        from sqlalchemy.orm.attributes import flag_modified
        flag_modified(survey, 'final_output')
        
        # Force the database to recognize the change
        db.flush()  # Flush changes to database without committing
        db.commit()
        
        # Verify the changes were saved
        db.refresh(survey)
        logger.info(f"ğŸ“Š [Survey API] After commit - sections count: {len(survey.final_output.get('sections', []))}")
        logger.info(f"ğŸ“Š [Survey API] After commit - section IDs: {[s.get('id') for s in survey.final_output.get('sections', [])]}")
        logger.info(f"ğŸ“Š [Survey API] After commit - section orders: {[s.get('order') for s in survey.final_output.get('sections', [])]}")
        
        logger.info(f"ğŸ’¾ [Survey API] Successfully reordered sections")
        return {"status": "success", "message": "Sections reordered successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"âŒ [Survey API] Failed to reorder sections: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to reorder sections: {str(e)}")


@router.put("/{survey_id}/questions/reorder")
async def reorder_questions(
    survey_id: UUID,
    request: QuestionReorderRequest,
    db: Session = Depends(get_db)
):
    """Reorder questions in a survey."""
    logger.info(f"ğŸ“ [Survey API] Reordering questions in survey {survey_id}")
    logger.info(f"ğŸ“¤ [Survey API] Received question order: {request.question_order}")
    
    try:
        # Get the survey
        survey = db.query(Survey).filter(Survey.id == survey_id).first()
        if not survey:
            raise HTTPException(status_code=404, detail="Survey not found")
        
        # Get survey data
        survey_data = survey.final_output or survey.raw_output or {}
        if not survey_data:
            raise HTTPException(status_code=400, detail="Survey has no data to update")
        
        logger.info(f"ğŸ“Š [Survey API] Survey data keys: {list(survey_data.keys())}")
        logger.info(f"ğŸ“Š [Survey API] Has questions key: {'questions' in survey_data}")
        logger.info(f"ğŸ“Š [Survey API] Has sections key: {'sections' in survey_data}")
        logger.info(f"ğŸ“Š [Survey API] Questions count: {len(survey_data.get('questions', []))}")
        logger.info(f"ğŸ“Š [Survey API] Sections count: {len(survey_data.get('sections', []))}")
        
        # Debug: Log the actual survey data structure
        if 'sections' in survey_data and survey_data['sections']:
            logger.info(f"ğŸ“Š [Survey API] First section: {survey_data['sections'][0]}")
            if 'questions' in survey_data['sections'][0]:
                logger.info(f"ğŸ“Š [Survey API] First section questions: {survey_data['sections'][0]['questions']}")
        
        # Handle questions within sections (new format)
        if 'sections' in survey_data and survey_data['sections']:
            logger.info(f"ğŸ“Š [Survey API] Processing questions within sections")
            
            # Create a mapping of question IDs to their current section and question data
            question_map = {}
            for section in survey_data['sections']:
                if 'questions' in section and section['questions']:
                    for question in section['questions']:
                        question_map[question.get('id')] = {
                            'question': question,
                            'section': section
                        }
            
            logger.info(f"ğŸ“Š [Survey API] Question map IDs: {list(question_map.keys())}")
            logger.info(f"ğŸ“Š [Survey API] Request question order: {request.question_order}")
            
            # Debug: Check if any questions from the request are in the map
            found_questions = []
            missing_questions = []
            for question_id in request.question_order:
                if question_id in question_map:
                    found_questions.append(question_id)
                else:
                    missing_questions.append(question_id)
            
            logger.info(f"ğŸ“Š [Survey API] Found questions: {found_questions}")
            logger.info(f"ğŸ“Š [Survey API] Missing questions: {missing_questions}")
            
            if not found_questions:
                logger.error(f"âŒ [Survey API] No questions from request found in question map!")
                raise HTTPException(status_code=404, detail="Question not found")
            
            # Group questions by section based on the new order
            section_question_groups = {}
            for question_id in request.question_order:
                if question_id in question_map:
                    section_id = question_map[question_id]['section'].get('id')
                    if section_id not in section_question_groups:
                        section_question_groups[section_id] = []
                    section_question_groups[section_id].append(question_map[question_id]['question'])
                    logger.info(f"ğŸ“Š [Survey API] Added question {question_id} to section {section_id}")
                else:
                    logger.warning(f"âš ï¸ [Survey API] Question ID {question_id} not found in question map!")
            
            # Update each section with its reordered questions
            for section in survey_data['sections']:
                section_id = section.get('id')
                if section_id in section_question_groups:
                    # Update order property for each question in this section
                    for index, question in enumerate(section_question_groups[section_id]):
                        question['order'] = index + 1
                        logger.info(f"ğŸ“Š [Survey API] Set question {question.get('id')} order to {index + 1} in section {section_id}")
                    
                    section['questions'] = section_question_groups[section_id]
                    logger.info(f"ğŸ“Š [Survey API] Updated section {section_id} with {len(section['questions'])} questions")
                else:
                    # If no questions for this section in the new order, keep original questions
                    logger.info(f"ğŸ“Š [Survey API] No questions in new order for section {section_id}, keeping original")
            
            logger.info(f"ğŸ“Š [Survey API] Successfully reordered questions within sections")
            
        # Handle legacy flat questions format
        elif 'questions' in survey_data and survey_data['questions']:
            logger.info(f"ğŸ“Š [Survey API] Processing legacy flat questions format")
            logger.info(f"ğŸ“Š [Survey API] Current questions before reorder: {[q.get('id') for q in survey_data.get('questions', [])]}")
            logger.info(f"ğŸ“Š [Survey API] Current question orders: {[q.get('order') for q in survey_data.get('questions', [])]}")
            
            # Create a mapping of question IDs to questions
            question_map = {question.get('id'): question for question in survey_data['questions']}
            logger.info(f"ğŸ“Š [Survey API] Question map IDs: {list(question_map.keys())}")
            
            # Reorder based on the provided order
            reordered_questions = []
            for question_id in request.question_order:
                if question_id in question_map:
                    reordered_questions.append(question_map[question_id])
                    logger.info(f"ğŸ“Š [Survey API] Added question {question_id} to reordered list")
                else:
                    logger.warning(f"âš ï¸ [Survey API] Question ID {question_id} not found in question map!")
            
            logger.info(f"ğŸ“Š [Survey API] Reordered questions count: {len(reordered_questions)}")
            
            # Update the order property for each question to reflect the new position
            for index, question in enumerate(reordered_questions):
                question['order'] = index + 1
                logger.info(f"ğŸ“Š [Survey API] Set question {question.get('id')} order to {index + 1}")
            
            survey_data['questions'] = reordered_questions
            logger.info(f"ğŸ“Š [Survey API] Questions after reorder: {[q.get('id') for q in reordered_questions]}")
            logger.info(f"ğŸ“Š [Survey API] Question orders: {[q.get('order') for q in reordered_questions]}")
            
        else:
            logger.warning(f"âš ï¸ [Survey API] No questions found in survey data!")
            logger.warning(f"âš ï¸ [Survey API] Survey data structure: {survey_data}")
            return {"status": "success", "message": "No questions to reorder"}
        
        # Create a completely new dictionary to force SQLAlchemy to detect the change
        new_final_output = dict(survey_data)
        
        # Deep copy sections if they exist
        if 'sections' in survey_data:
            new_final_output['sections'] = []
            for section in survey_data['sections']:
                new_section = dict(section)
                if 'questions' in section:
                    new_section['questions'] = [dict(question) for question in section['questions']]
                new_final_output['sections'].append(new_section)
        
        # Deep copy questions if they exist (legacy format)
        if 'questions' in survey_data:
            new_final_output['questions'] = [dict(question) for question in survey_data['questions']]
        
        # Save the updated survey data
        survey.final_output = new_final_output
        survey.updated_at = datetime.now()
        
        # Explicitly tell SQLAlchemy that the JSONB field has changed
        from sqlalchemy.orm.attributes import flag_modified
        flag_modified(survey, 'final_output')
        
        # Force the database to recognize the change
        db.flush()  # Flush changes to database without committing
        db.commit()
        
        # Sync to golden pair if this is a reference survey
        if survey.status == "reference":
            try:
                from src.services.golden_service import GoldenService
                golden_service = GoldenService(db)
                golden_service.sync_survey_to_golden_pair(survey.id, survey.final_output)
                logger.info(f"âœ… [Survey API] Synced question reorder to golden pair")
            except Exception as e:
                logger.warning(f"âš ï¸ [Survey API] Failed to sync question reorder to golden pair: {e}")
                # Don't fail the edit if sync fails
        
        # Verify the changes were saved
        db.refresh(survey)
        if 'sections' in survey.final_output:
            logger.info(f"ğŸ“Š [Survey API] After commit - sections count: {len(survey.final_output.get('sections', []))}")
            for i, section in enumerate(survey.final_output.get('sections', [])):
                logger.info(f"ğŸ“Š [Survey API] Section {i+1} - questions count: {len(section.get('questions', []))}")
                logger.info(f"ğŸ“Š [Survey API] Section {i+1} - question IDs: {[q.get('id') for q in section.get('questions', [])]}")
                logger.info(f"ğŸ“Š [Survey API] Section {i+1} - question orders: {[q.get('order') for q in section.get('questions', [])]}")
        else:
            logger.info(f"ğŸ“Š [Survey API] After commit - questions count: {len(survey.final_output.get('questions', []))}")
            logger.info(f"ğŸ“Š [Survey API] After commit - question IDs: {[q.get('id') for q in survey.final_output.get('questions', [])]}")
            logger.info(f"ğŸ“Š [Survey API] After commit - question orders: {[q.get('order') for q in survey.final_output.get('questions', [])]}")
        
        logger.info(f"ğŸ’¾ [Survey API] Successfully reordered questions")
        return {"status": "success", "message": "Questions reordered successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"âŒ [Survey API] Failed to reorder questions: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to reorder questions: {str(e)}")


@router.put("/{survey_id}/sections/{section_id}")
async def update_section(
    survey_id: UUID,
    section_id: int,
    section_update: SectionUpdate,
    db: Session = Depends(get_db)
):
    """Update a specific section in a survey."""
    logger.info(f"ğŸ“ [Survey API] Updating section {section_id} in survey {survey_id}")
    
    try:
        # Get the survey
        survey = db.query(Survey).filter(Survey.id == survey_id).first()
        if not survey:
            raise HTTPException(status_code=404, detail="Survey not found")
        
        # Get survey data
        survey_data = survey.final_output or survey.raw_output or {}
        if not survey_data:
            raise HTTPException(status_code=400, detail="Survey has no data to update")
        
        # Find and update the section
        section_found = False
        
        if 'sections' in survey_data and survey_data['sections']:
            for i, section in enumerate(survey_data['sections']):
                if section.get('id') == section_id:
                    # Update the section with provided fields
                    for field, value in section_update.dict(exclude_unset=True).items():
                        section[field] = value
                    section_found = True
                    logger.info(f"âœ… [Survey API] Updated section {section_id}")
                    break
        
        if not section_found:
            raise HTTPException(status_code=404, detail="Section not found")
        
        # Create a completely new dictionary to force SQLAlchemy to detect the change
        new_final_output = dict(survey_data)
        
        # Deep copy sections if they exist
        if 'sections' in survey_data:
            new_final_output['sections'] = []
            for section in survey_data['sections']:
                new_section = dict(section)
                if 'questions' in section:
                    new_section['questions'] = [dict(question) for question in section['questions']]
                new_final_output['sections'].append(new_section)
        
        # Deep copy questions if they exist (legacy format)
        if 'questions' in survey_data:
            new_final_output['questions'] = [dict(question) for question in survey_data['questions']]
        
        # Save the updated survey data
        survey.final_output = new_final_output
        survey.updated_at = datetime.now()
        
        # Explicitly tell SQLAlchemy that the JSONB field has changed
        from sqlalchemy.orm.attributes import flag_modified
        flag_modified(survey, 'final_output')
        
        # Force the database to recognize the change
        db.flush()  # Flush changes to database without committing
        db.commit()
        
        # Sync to golden pair if this is a reference survey
        if survey.status == "reference":
            try:
                from src.services.golden_service import GoldenService
                golden_service = GoldenService(db)
                golden_service.sync_survey_to_golden_pair(survey.id, new_final_output)
                logger.info(f"âœ… [Survey API] Synced section update to golden pair")
            except Exception as e:
                logger.warning(f"âš ï¸ [Survey API] Failed to sync section update to golden pair: {e}")
                # Don't fail the edit if sync fails
        
        logger.info(f"ğŸ’¾ [Survey API] Successfully saved updated section {section_id}")
        return {"status": "success", "message": "Section updated successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"âŒ [Survey API] Failed to update section {section_id}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to update section: {str(e)}")


@router.post("/{survey_id}/sections")
async def create_section(
    survey_id: UUID,
    section: SurveySection,
    db: Session = Depends(get_db)
):
    """Create a new section in a survey."""
    logger.info(f"ğŸ“ [Survey API] Creating new section in survey {survey_id}")
    
    try:
        # Get the survey
        survey = db.query(Survey).filter(Survey.id == survey_id).first()
        if not survey:
            raise HTTPException(status_code=404, detail="Survey not found")
        
        # Get survey data
        survey_data = survey.final_output or survey.raw_output or {}
        if not survey_data:
            raise HTTPException(status_code=400, detail="Survey has no data to update")
        
        # Initialize sections if they don't exist
        if 'sections' not in survey_data:
            survey_data['sections'] = []
        
        # Add the new section
        survey_data['sections'].append(section.dict())
        
        # Save the updated survey data
        survey.final_output = survey_data
        survey.updated_at = datetime.now()
        db.commit()
        
        logger.info(f"ğŸ’¾ [Survey API] Successfully created new section")
        return {"status": "success", "message": "Section created successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"âŒ [Survey API] Failed to create section: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to create section: {str(e)}")


@router.delete("/{survey_id}/sections/{section_id}")
async def delete_section(
    survey_id: UUID,
    section_id: int,
    db: Session = Depends(get_db)
):
    """Delete a section from a survey."""
    logger.info(f"ğŸ“ [Survey API] Deleting section {section_id} from survey {survey_id}")
    
    try:
        # Get the survey
        survey = db.query(Survey).filter(Survey.id == survey_id).first()
        if not survey:
            raise HTTPException(status_code=404, detail="Survey not found")
        
        # Get survey data
        survey_data = survey.final_output or survey.raw_output or {}
        if not survey_data:
            raise HTTPException(status_code=400, detail="Survey has no data to update")
        
        # Find and remove the section
        section_found = False
        
        if 'sections' in survey_data and survey_data['sections']:
            for i, section in enumerate(survey_data['sections']):
                if section.get('id') == section_id:
                    survey_data['sections'].pop(i)
                    section_found = True
                    logger.info(f"âœ… [Survey API] Deleted section {section_id}")
                    break
        
        if not section_found:
            raise HTTPException(status_code=404, detail="Section not found")
        
        # Save the updated survey data
        survey.final_output = survey_data
        survey.updated_at = datetime.now()
        db.commit()
        
        logger.info(f"ğŸ’¾ [Survey API] Successfully deleted section {section_id}")
        return {"status": "success", "message": "Section deleted successfully"}
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"âŒ [Survey API] Failed to delete section {section_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")


class StructureValidationRequest(BaseModel):
    """Request model for structure validation"""
    survey_data: Dict[str, Any]
    methodology_tags: Optional[List[str]] = None
    industry: Optional[str] = None
    respondent_type: Optional[str] = None


@router.post("/{survey_id}/structure-validate")
async def validate_survey_structure(
    survey_id: str,
    request: StructureValidationRequest,
    db: Session = Depends(get_db)
):
    """
    Validate survey structure using QNR labeling framework
    Returns quality score and flagged issues (non-blocking)
    """
    try:
        logger.info(f"ğŸ” [Survey API] Starting structure validation for survey {survey_id}")
        
        # Initialize structure validator
        validator = SurveyStructureValidator(db)
        
        # Prepare RFQ context
        rfq_context = {
            'methodology_tags': request.methodology_tags or [],
            'industry': request.industry,
            'respondent_type': request.respondent_type
        }
        
        # Validate structure
        validation_report = await validator.validate_structure(
            survey_json=request.survey_data,
            rfq_context=rfq_context
        )
        
        # Convert to API response format
        response = {
            'survey_id': survey_id,
            'structure_validation': validation_report.to_dict(),
            'summary': validation_report.get_summary(),
            'is_high_quality': validation_report.is_high_quality(),
            'has_critical_issues': validation_report.has_critical_issues(),
            'flagged_for_review': validation_report.has_critical_issues(),
            'flag_reason': validation_report.get_critical_issues_summary() if validation_report.has_critical_issues() else None
        }
        
        logger.info(f"âœ… [Survey API] Structure validation completed: {validation_report.get_summary()}")
        return response
        
    except Exception as e:
        logger.error(f"âŒ [Survey API] Structure validation failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Structure validation failed: {str(e)}")


@router.get("/{survey_id}/structure-validate")
async def get_survey_structure_validation(
    survey_id: str,
    db: Session = Depends(get_db)
):
    """
    Get structure validation results for an existing survey
    """
    try:
        logger.info(f"ğŸ” [Survey API] Getting structure validation for survey {survey_id}")
        
        # Get survey from database
        survey = db.query(Survey).filter(Survey.id == survey_id).first()
        if not survey:
            raise HTTPException(status_code=404, detail="Survey not found")
        
        if not survey.final_output:
            raise HTTPException(status_code=400, detail="Survey has no final output to validate")
        
        # Initialize structure validator
        validator = SurveyStructureValidator(db)
        
        # Prepare RFQ context (extract from survey metadata if available)
        rfq_context = {
            'methodology_tags': getattr(survey, 'methodology_tags', []) or [],
            'industry': getattr(survey, 'industry_category', None),
            'respondent_type': getattr(survey, 'respondent_type', None)
        }
        
        # Validate structure
        validation_report = await validator.validate_structure(
            survey_json=survey.final_output,
            rfq_context=rfq_context
        )
        
        # Convert to API response format
        response = {
            'survey_id': survey_id,
            'structure_validation': validation_report.to_dict(),
            'summary': validation_report.get_summary(),
            'is_high_quality': validation_report.is_high_quality(),
            'has_critical_issues': validation_report.has_critical_issues(),
            'flagged_for_review': validation_report.has_critical_issues(),
            'flag_reason': validation_report.get_critical_issues_summary() if validation_report.has_critical_issues() else None
        }
        
        logger.info(f"âœ… [Survey API] Structure validation retrieved: {validation_report.get_summary()}")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [Survey API] Failed to get structure validation: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get structure validation: {str(e)}")


@router.get("/{survey_id}/llm-audits")
async def get_survey_llm_audits(
    survey_id: UUID,
    db: Session = Depends(get_db)
) -> List[Dict[str, Any]]:
    """
    Get all LLM audit records for a specific survey.
    Returns chronologically ordered list of all AI interactions including generation, evaluations, and RFQ parsing.
    """
    logger.info(f"ğŸ” [Survey API] Fetching LLM audits for survey: {survey_id}")
    
    try:
        # Verify survey exists
        survey = db.query(Survey).filter(Survey.id == survey_id).first()
        if not survey:
            raise HTTPException(status_code=404, detail="Survey not found")
        
        # Query all LLM audit records for this survey
        audit_records = db.query(LLMAudit).filter(
            LLMAudit.parent_survey_id == str(survey_id)
        ).order_by(LLMAudit.created_at.asc()).all()
        
        logger.info(f"ğŸ“Š [Survey API] Found {len(audit_records)} LLM audit records directly linked to survey {survey_id}")
        
        # Also query RFQ parsing audits if the survey has an RFQ
        if survey.rfq_id:
            # Query audits linked by parent_rfq_id
            rfq_parsing_audits = db.query(LLMAudit).filter(
                LLMAudit.parent_rfq_id == survey.rfq_id
            ).order_by(LLMAudit.created_at.asc()).all()
            
            logger.info(f"ğŸ“Š [Survey API] Found {len(rfq_parsing_audits)} RFQ parsing audit records for RFQ {survey.rfq_id}")
            
            # Combine both sets of audits and sort by timestamp
            all_audit_ids = set(str(r.id) for r in audit_records)
            for rfq_audit in rfq_parsing_audits:
                if str(rfq_audit.id) not in all_audit_ids:
                    audit_records.append(rfq_audit)
                    all_audit_ids.add(str(rfq_audit.id))
            
            # Try to find document parsing audits by looking at the RFQ
            from src.database.models import RFQ
            rfq_obj = db.query(RFQ).filter(RFQ.id == survey.rfq_id).first()
            
            # Check if RFQ was created from a document upload
            is_document_upload = False
            if rfq_obj and rfq_obj.document_upload_id:
                is_document_upload = True
                logger.info(f"ğŸ” [Survey API] RFQ has document_upload_id: {rfq_obj.document_upload_id}")
            elif rfq_obj and rfq_obj.enhanced_rfq_data:
                # Check if rfq_data indicates document upload
                doc_source = rfq_obj.enhanced_rfq_data.get('document_source')
                if doc_source and doc_source.get('type') == 'upload':
                    is_document_upload = True
                    logger.info(f"ğŸ” [Survey API] RFQ was created from document upload: {doc_source.get('filename')}")
            
            if is_document_upload and rfq_obj:
                # Query for audits created around the time of document parsing
                # Document parsing audits don't have parent_rfq_id set, so we look by purpose
                # and approximate timestamp matching (within 30 minutes before RFQ creation)
                from datetime import timedelta
                if rfq_obj.created_at:
                    # Find document parsing audits created before this RFQ (doc parsing happens first)
                    doc_parsing_audits = db.query(LLMAudit).filter(
                        LLMAudit.purpose == "document_parsing",
                        LLMAudit.created_at <= rfq_obj.created_at,
                        LLMAudit.created_at >= rfq_obj.created_at - timedelta(hours=24)  # Look back up to 24 hours
                    ).order_by(LLMAudit.created_at.desc()).limit(10).all()  # Get most recent 10
                    
                    logger.info(f"ğŸ“Š [Survey API] Found {len(doc_parsing_audits)} document parsing audit records near RFQ creation time")
                    
                    for doc_audit in doc_parsing_audits:
                        if str(doc_audit.id) not in all_audit_ids:
                            audit_records.append(doc_audit)
                            all_audit_ids.add(str(doc_audit.id))
            
            # Re-sort by created_at after combining
            audit_records.sort(key=lambda r: r.created_at)
        
        logger.info(f"ğŸ“Š [Survey API] Total audit records including RFQ parsing: {len(audit_records)}")
        
        # Helper function to parse raw_response
        def parse_raw_response(raw_response: str) -> Any:
            """Parse raw_response from string to appropriate type"""
            if not raw_response:
                return None
            try:
                # If it's already a dict/list, return it
                if isinstance(raw_response, (dict, list)):
                    return raw_response
                # Try to parse as JSON
                return json.loads(raw_response)
            except (json.JSONDecodeError, TypeError):
                # If not JSON, return as string
                return raw_response
        
        # Convert to response format
        audit_responses = []
        for record in audit_records:
            audit_responses.append({
                "id": str(record.id),
                "interaction_id": record.interaction_id,
                "parent_workflow_id": record.parent_workflow_id,
                "parent_survey_id": record.parent_survey_id,
                "parent_rfq_id": str(record.parent_rfq_id) if record.parent_rfq_id else None,
                "model_name": record.model_name,
                "model_provider": record.model_provider,
                "model_version": record.model_version,
                "purpose": record.purpose,
                "sub_purpose": record.sub_purpose,
                "context_type": record.context_type,
                "input_prompt": record.input_prompt,
                "input_tokens": record.input_tokens,
                "output_content": record.output_content,
                "output_tokens": record.output_tokens,
                "raw_response": parse_raw_response(record.raw_response),
                "temperature": float(record.temperature) if record.temperature else None,
                "top_p": float(record.top_p) if record.top_p else None,
                "max_tokens": record.max_tokens,
                "frequency_penalty": float(record.frequency_penalty) if record.frequency_penalty else None,
                "presence_penalty": float(record.presence_penalty) if record.presence_penalty else None,
                "stop_sequences": record.stop_sequences,
                "response_time_ms": record.response_time_ms,
                "cost_usd": float(record.cost_usd) if record.cost_usd else None,
                "success": record.success,
                "error_message": record.error_message,
                "interaction_metadata": record.interaction_metadata,
                "tags": record.tags,
                "created_at": record.created_at.isoformat() if record.created_at else "",
                "updated_at": record.updated_at.isoformat() if record.updated_at else ""
            })
        
        logger.info(f"âœ… [Survey API] Returning {len(audit_responses)} LLM audit records")
        return audit_responses
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [Survey API] Failed to fetch LLM audits: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch LLM audits: {str(e)}")


class ReferenceExamplesResponse(BaseModel):
    survey_title: str
    eight_questions_tab: Dict[str, Any]
    manual_comment_digest_tab: Dict[str, Any]
    golden_examples_tab: Dict[str, Any]
    golden_sections_tab: Dict[str, Any]


@router.get("/{survey_id}/reference-examples", response_model=ReferenceExamplesResponse)
async def get_survey_reference_examples(
    survey_id: UUID,
    db: Session = Depends(get_db)
):
    """
    Get reference examples (golden questions with comments) and feedback digest used during survey generation
    """
    try:
        from src.database.models import GoldenQuestion, QuestionAnnotation
        from src.services.rule_based_multi_level_rag_service import RuleBasedMultiLevelRAGService
        from src.utils.database_session_manager import DatabaseSessionManager
        
        # Get survey
        survey = db.query(Survey).filter(Survey.id == survey_id).first()
        if not survey:
            raise HTTPException(status_code=404, detail="Survey not found")
        
        # Get survey title from final_output
        survey_title = "Untitled Survey"
        if survey.final_output:
            survey_title = survey.final_output.get("title", survey_title)
        
        # ===== 8 QUESTIONS TAB =====
        used_question_ids = survey.used_golden_questions or []
        eight_questions = []
        eight_questions_prompt_text = ""
        
        if used_question_ids:
            # Fetch questions ordered the same way as retrieval service (human_verified desc, quality_score desc)
            # This matches the order used by the prompt builder
            # Include AI-generated questions IF human_verified == True
            questions = DatabaseSessionManager.safe_query(
                db,
                lambda: db.query(GoldenQuestion)
                .filter(GoldenQuestion.id.in_(used_question_ids))
                .order_by(GoldenQuestion.human_verified.desc(), GoldenQuestion.quality_score.desc())
                .limit(8)  # Limit to 8 to match what prompt builder uses (golden_questions[:8])
                .all(),
                fallback_value=[],
                operation_name="get golden questions for reference examples"
            )
            
            # Build question data and format for prompt
            question_data_for_formatting = []
            for question in questions:
                # Fetch annotation comment if available
                annotation_comment = None
                quality_score = float(question.quality_score) if question.quality_score else 0.5
                human_verified = question.human_verified
                
                if question.annotation_id:
                    annotation = DatabaseSessionManager.safe_query(
                        db,
                        lambda: db.query(QuestionAnnotation)
                        .filter(QuestionAnnotation.id == question.annotation_id)
                        .first(),
                        fallback_value=None,
                        operation_name="fetch annotation comment"
                    )
                    if annotation and annotation.comment:
                        annotation_comment = annotation.comment
                
                question_dict = {
                    "id": str(question.id),
                    "question_text": question.question_text,
                    "question_type": question.question_type or "unknown",
                    "annotation_comment": annotation_comment,
                    "quality_score": quality_score,
                    "human_verified": human_verified
                }
                
                eight_questions.append(question_dict)
                question_data_for_formatting.append(question_dict)
            
            # Format prompt text using shared helper
            from src.utils.prompt_formatters import format_golden_questions_for_prompt
            formatted_lines = format_golden_questions_for_prompt(question_data_for_formatting)
            eight_questions_prompt_text = "\n".join(formatted_lines)
        
        # ===== MANUAL COMMENT DIGEST TAB =====
        feedback_digest_data = survey.feedback_digest
        is_reconstructed = False
        manual_comment_digest_questions = []
        manual_comment_digest_prompt_text = ""
        
        if not feedback_digest_data:
            # Reconstruct feedback digest from used questions
            logger.info(f"ğŸ“ [Survey API] Reconstructing feedback digest for survey {survey_id}")
            rag_service = RuleBasedMultiLevelRAGService(db)
            
            # Get methodology tags and industry from RFQ if available
            methodology_tags = None
            industry = None
            if survey.rfq_id:
                from src.database.models import RFQ
                rfq = db.query(RFQ).filter(RFQ.id == survey.rfq_id).first()
                if rfq and rfq.enhanced_rfq_data:
                    methodology_tags = rfq.enhanced_rfq_data.get("methodology_tags")
                    industry = rfq.enhanced_rfq_data.get("industry_category")
            
            feedback_digest_data = await rag_service.get_feedback_digest(
                methodology_tags=methodology_tags,
                industry=industry,
                limit=50
            )
            is_reconstructed = True
        
        # Get questions from feedback digest (human comments only)
        if feedback_digest_data:
            questions_with_feedback = feedback_digest_data.get('questions_with_feedback', [])
            for feedback_item in questions_with_feedback[:50]:  # Limit to 50
                manual_comment_digest_questions.append({
                    "id": feedback_item.get('id', ''),
                    "question_text": feedback_item.get('question_text', ''),
                    "question_type": feedback_item.get('question_type', 'unknown'),
                    "annotation_comment": feedback_item.get('comment', ''),
                    "quality_score": feedback_item.get('quality_score', 0.5),
                    "human_verified": feedback_item.get('human_verified', False)
                })
            
            manual_comment_digest_prompt_text = feedback_digest_data.get("feedback_digest", "No manual comment digest available.")
        
        # ===== GOLDEN EXAMPLES (RFQ-SURVEY PAIRS) TAB =====
        used_golden_example_ids = survey.used_golden_examples or []
        golden_examples = []
        golden_examples_prompt_text = ""
        
        if used_golden_example_ids:
            from src.database.models import GoldenRFQSurveyPair
            
            # Fetch golden RFQ-survey pairs
            golden_pairs = DatabaseSessionManager.safe_query(
                db,
                lambda: db.query(GoldenRFQSurveyPair)
                .filter(GoldenRFQSurveyPair.id.in_(used_golden_example_ids))
                .limit(3)  # Limit to 3 as used in prompt builder
                .all(),
                fallback_value=[],
                operation_name="get golden RFQ-survey pairs for reference examples"
            )
            
            for pair in golden_pairs:
                # Extract survey title from survey_json
                pair_title = pair.title or "Untitled Golden Example"
                survey_title_from_json = pair_title
                if pair.survey_json and isinstance(pair.survey_json, dict):
                    if "final_output" in pair.survey_json and isinstance(pair.survey_json["final_output"], dict):
                        survey_title_from_json = pair.survey_json["final_output"].get("title", pair_title)
                    elif "title" in pair.survey_json:
                        survey_title_from_json = pair.survey_json.get("title", pair_title)
                
                golden_examples.append({
                    "id": str(pair.id),
                    "title": pair_title,
                    "rfq_text": pair.rfq_text[:500] + "..." if pair.rfq_text and len(pair.rfq_text) > 500 else (pair.rfq_text or ""),
                    "survey_title": survey_title_from_json,
                    "methodology_tags": pair.methodology_tags or [],
                    "industry_category": pair.industry_category,
                    "research_goal": pair.research_goal,
                    "quality_score": float(pair.quality_score) if pair.quality_score else 0.5,
                    "human_verified": pair.human_verified,
                    "usage_count": pair.usage_count or 0
                })
            
            # Format prompt text (how it appears in the prompt)
            if golden_examples:
                prompt_lines = [
                    "## ğŸ“‹ GOLDEN RFQ-SURVEY PAIRS:",
                    "",
                    "Complete examples of high-quality RFQ-to-survey transformations. Use these as reference for structure, flow, and quality standards.",
                    ""
                ]
                
                for i, example in enumerate(golden_examples, 1):
                    prompt_lines.extend([
                        f"### Example {i}: {example['title']}",
                        "",
                        f"**RFQ:**",
                        f"{example['rfq_text'][:300]}..." if len(example['rfq_text']) > 300 else example['rfq_text'],
                        "",
                        f"**Survey Title:** {example['survey_title']}",
                        f"**Methodology:** {', '.join(example['methodology_tags']) if example['methodology_tags'] else 'N/A'}",
                        f"**Industry:** {example['industry_category'] or 'N/A'}",
                        f"**Quality Score:** {example['quality_score']:.2f}/1.0",
                        ""
                    ])
                
                golden_examples_prompt_text = "\n".join(prompt_lines)
        
        # ===== GOLDEN SECTIONS TAB =====
        used_golden_section_ids = survey.used_golden_sections or []
        golden_sections = []
        golden_sections_prompt_text = ""
        
        if used_golden_section_ids:
            from src.database.models import GoldenSection
            
            # Fetch golden sections
            sections = DatabaseSessionManager.safe_query(
                db,
                lambda: db.query(GoldenSection)
                .filter(GoldenSection.id.in_(used_golden_section_ids))
                .limit(5)  # Limit to 5 as used in prompt builder
                .all(),
                fallback_value=[],
                operation_name="get golden sections for reference examples"
            )
            
            for section in sections:
                golden_sections.append({
                    "id": str(section.id),
                    "section_id": section.section_id,
                    "section_title": section.section_title or "Untitled Section",
                    "section_text": section.section_text[:500] + "..." if section.section_text and len(section.section_text) > 500 else (section.section_text or ""),
                    "section_type": section.section_type or "unknown",
                    "methodology_tags": section.methodology_tags or [],
                    "industry_keywords": section.industry_keywords or [],
                    "quality_score": float(section.quality_score) if section.quality_score else 0.5,
                    "human_verified": section.human_verified
                })
            
            # Format prompt text (how it appears in the prompt)
            if golden_sections:
                prompt_lines = [
                    "# 4.2.5 EXPERT SECTION EXAMPLES",
                    "",
                    "High-quality sections from verified surveys - use as templates for section structure and flow:",
                    ""
                ]
                
                for i, section in enumerate(golden_sections, 1):
                    truncated_text = section['section_text'][:300] + "..." if len(section['section_text']) > 300 else section['section_text']
                    prompt_lines.extend([
                        f"**Section Example {i}:** {section['section_title']}",
                        f"**Type:** {section['section_type']} | **Quality:** {section['quality_score']:.2f}/1.0 | {'âœ… Verified' if section['human_verified'] else 'ğŸ¤– AI'}",
                    ])
                    
                    if section['methodology_tags']:
                        prompt_lines.append(f"**Methodology:** {', '.join(section['methodology_tags'])}")
                    
                    prompt_lines.extend([
                        f"**Section Text:** {truncated_text}",
                        ""
                    ])
                
                prompt_lines.extend([
                    "**USAGE INSTRUCTIONS:**",
                    "- Use these sections as templates for structure, flow, and question organization",
                    "- Pay attention to how questions are grouped and sequenced within sections",
                    "- Adapt section titles and intro text to match your RFQ context",
                    ""
                ])
                
                golden_sections_prompt_text = "\n".join(prompt_lines)
        
        return ReferenceExamplesResponse(
            survey_title=survey_title,
            eight_questions_tab={
                "prompt_text": eight_questions_prompt_text,
                "questions": eight_questions
            },
            manual_comment_digest_tab={
                "prompt_text": manual_comment_digest_prompt_text,
                "questions": manual_comment_digest_questions,
                "is_reconstructed": is_reconstructed,
                "total_feedback_count": feedback_digest_data.get("total_feedback_count", 0) if feedback_digest_data else 0
            },
            golden_examples_tab={
                "prompt_text": golden_examples_prompt_text,
                "examples": golden_examples
            },
            golden_sections_tab={
                "prompt_text": golden_sections_prompt_text,
                "sections": golden_sections
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ [Survey API] Failed to fetch reference examples: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to fetch reference examples: {str(e)}")